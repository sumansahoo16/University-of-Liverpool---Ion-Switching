{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Based on  https://www.kaggle.com/siavrez/wavenet-keras and https://www.kaggle.com/ragnar123/wavenet-with-1-more-feature\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Input, Dense, Add, Multiply\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "\"\"\" TF-Keras SWA: callback utility for performing stochastic weight averaging (SWA).\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "class SWA(Callback):\n",
    "    \"\"\" Stochastic Weight Averging.\n",
    "    # Paper\n",
    "        title: Averaging Weights Leads to Wider Optima and Better Generalization\n",
    "        link: https://arxiv.org/abs/1803.05407\n",
    "    # Arguments\n",
    "        start_epoch:   integer, epoch when swa should start.\n",
    "        lr_schedule:   string, type of learning rate schedule.\n",
    "        swa_lr:        float, learning rate for swa sampling.\n",
    "        swa_lr2:       float, upper bound of cyclic learning rate.\n",
    "        swa_freq:      integer, length of learning rate cycle.\n",
    "        verbose:       integer, verbosity mode, 0 or 1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 start_epoch,\n",
    "                 lr_schedule='manual',\n",
    "                 swa_lr='auto',\n",
    "                 swa_lr2='auto',\n",
    "                 swa_freq=1,\n",
    "                 verbose=0):\n",
    "                 \n",
    "        super(SWA, self).__init__()\n",
    "        self.start_epoch = start_epoch - 1\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.swa_lr = swa_lr\n",
    "        self.swa_lr2 = swa_lr2\n",
    "        self.swa_freq = swa_freq\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if start_epoch < 2:\n",
    "            raise ValueError('\"swa_start\" attribute cannot be lower than 2.')\n",
    "\n",
    "        schedules = ['manual', 'constant', 'cyclic']\n",
    "\n",
    "        if self.lr_schedule not in schedules:\n",
    "            raise ValueError('\"{}\" is not a valid learning rate schedule' \\\n",
    "                             .format(self.lr_schedule))\n",
    "\n",
    "        if self.lr_schedule == 'cyclic' and self.swa_freq < 2:\n",
    "            raise ValueError('\"swa_freq\" must be higher than 1 for cyclic schedule.')\n",
    "\n",
    "        if self.swa_lr == 'auto' and self.swa_lr2 != 'auto':\n",
    "            raise ValueError('\"swa_lr2\" cannot be manually set if \"swa_lr\" is automatic.') \n",
    "            \n",
    "        if self.lr_schedule == 'cyclic' and self.swa_lr != 'auto' \\\n",
    "           and self.swa_lr2 != 'auto' and self.swa_lr > self.swa_lr2:\n",
    "            raise ValueError('\"swa_lr\" must be lower than \"swa_lr2\".')\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.epochs = self.params.get('epochs')\n",
    "\n",
    "        if self.start_epoch >= self.epochs - 1:\n",
    "            raise ValueError('\"swa_start\" attribute must be lower than \"epochs\".')\n",
    "\n",
    "        self.init_lr = K.eval(self.model.optimizer.lr)\n",
    "\n",
    "        # automatic swa_lr\n",
    "        if self.swa_lr == 'auto':\n",
    "            self.swa_lr = 0.1*self.init_lr\n",
    "        \n",
    "        if self.init_lr < self.swa_lr:\n",
    "            raise ValueError('\"swa_lr\" must be lower than rate set in optimizer.')\n",
    "\n",
    "        # automatic swa_lr2 between initial lr and swa_lr   \n",
    "        if self.lr_schedule == 'cyclic' and self.swa_lr2 == 'auto':\n",
    "            self.swa_lr2 = self.swa_lr + (self.init_lr - self.swa_lr)*0.25\n",
    "\n",
    "        self._check_batch_norm()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "\n",
    "        self.current_epoch = epoch\n",
    "        self._scheduler(epoch)\n",
    "\n",
    "        # constant schedule is updated epoch-wise\n",
    "        if self.lr_schedule == 'constant' or self.is_batch_norm_epoch:\n",
    "            self._update_lr(epoch)\n",
    "\n",
    "        if self.is_swa_start_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: starting stochastic weight averaging'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "            self._set_swa_weights(epoch)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: reinitializing batch normalization layers'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "            self._reset_batch_norm()\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: running forward pass to adjust batch normalization'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "\n",
    "        # update lr each batch for cyclic lr schedule\n",
    "        if self.lr_schedule == 'cyclic':\n",
    "            self._update_lr(self.current_epoch, batch)\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "\n",
    "            batch_size = self.params['samples']\n",
    "            momentum = batch_size / (batch*batch_size + batch_size)\n",
    "\n",
    "            for layer in self.batch_norm_layers:\n",
    "                layer.momentum = momentum\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.eval(self.model.optimizer.lr)\n",
    "        for k, v in logs.items():\n",
    "            if k == 'lr':\n",
    "                self.model.history.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        if self.is_swa_start_epoch:\n",
    "            self.swa_start_epoch = epoch\n",
    "\n",
    "        if self.is_swa_epoch and not self.is_batch_norm_epoch:\n",
    "            self.swa_weights = self._average_weights(epoch)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "\n",
    "        if not self.has_batch_norm:\n",
    "            self._set_swa_weights(self.epochs)\n",
    "        else:\n",
    "            self._restore_batch_norm()\n",
    "\n",
    "    def _scheduler(self, epoch):\n",
    "\n",
    "        swa_epoch = (epoch - self.start_epoch)\n",
    "\n",
    "        self.is_swa_epoch = epoch >= self.start_epoch and swa_epoch % self.swa_freq == 0\n",
    "        self.is_swa_start_epoch = epoch == self.start_epoch\n",
    "        self.is_batch_norm_epoch = epoch == self.epochs - 1 and self.has_batch_norm\n",
    "\n",
    "    def _average_weights(self, epoch):\n",
    "\n",
    "        return [(swa_w * (epoch - self.start_epoch) + w)\n",
    "                / ((epoch - self.start_epoch) + 1)\n",
    "                for swa_w, w in zip(self.swa_weights, self.model.get_weights())]\n",
    "\n",
    "    def _update_lr(self, epoch, batch=None):\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "            lr = 0\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        elif self.lr_schedule == 'constant':\n",
    "            lr = self._constant_schedule(epoch)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        elif self.lr_schedule == 'cyclic':\n",
    "            lr = self._cyclic_schedule(epoch, batch)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def _constant_schedule(self, epoch):\n",
    "\n",
    "        t = epoch / self.start_epoch\n",
    "        lr_ratio = self.swa_lr / self.init_lr\n",
    "        if t <= 0.5:\n",
    "            factor = 1.0\n",
    "        elif t <= 0.9:\n",
    "            factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
    "        else:\n",
    "            factor = lr_ratio\n",
    "        return self.init_lr * factor\n",
    "\n",
    "    def _cyclic_schedule(self, epoch, batch):\n",
    "        \"\"\" Designed after Section 3.1 of Averaging Weights Leads to\n",
    "        Wider Optima and Better Generalization(https://arxiv.org/abs/1803.05407)\n",
    "        \"\"\"\n",
    "        # steps are mini-batches per epoch, equal to training_samples / batch_size\n",
    "        steps = self.params.get('steps')\n",
    "        \n",
    "        #occasionally steps parameter will not be set. We then calculate it ourselves\n",
    "        if steps == None:\n",
    "            steps = self.params['samples'] // self.params['batch_size']\n",
    "        \n",
    "        swa_epoch = (epoch - self.start_epoch) % self.swa_freq\n",
    "        cycle_length = self.swa_freq * steps\n",
    "\n",
    "        # batch 0 indexed, so need to add 1\n",
    "        i = (swa_epoch * steps) + (batch + 1)\n",
    "        if epoch >= self.start_epoch:\n",
    "            t = (((i-1) % cycle_length) + 1)/cycle_length\n",
    "            return (1-t)*self.swa_lr2 + t*self.swa_lr\n",
    "        else:\n",
    "            return self._constant_schedule(epoch)\n",
    "\n",
    "    def _set_swa_weights(self, epoch):\n",
    "\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: final model weights set to stochastic weight average'\n",
    "                  % (epoch + 1))\n",
    "\n",
    "    def _check_batch_norm(self):\n",
    "\n",
    "        self.batch_norm_momentums = []\n",
    "        self.batch_norm_layers = []\n",
    "        self.has_batch_norm = False\n",
    "        self.running_bn_epoch = False\n",
    "\n",
    "        for layer in self.model.layers:\n",
    "            if issubclass(layer.__class__, BatchNormalization):\n",
    "                self.has_batch_norm = True\n",
    "                self.batch_norm_momentums.append(layer.momentum)\n",
    "                self.batch_norm_layers.append(layer)\n",
    "\n",
    "        if self.verbose > 0 and self.has_batch_norm:\n",
    "            print('Model uses batch normalization. SWA will require last epoch '\n",
    "                  'to be a forward pass and will run with no learning rate')\n",
    "\n",
    "    def _reset_batch_norm(self):\n",
    "\n",
    "        for layer in self.batch_norm_layers:\n",
    "\n",
    "            # to get properly initialized moving mean and moving variance weights\n",
    "            # we initialize a new batch norm layer from the config of the existing\n",
    "            # layer, build that layer, retrieve its reinitialized moving mean and\n",
    "            # moving var weights and then delete the layer\n",
    "            bn_config = layer.get_config()\n",
    "            new_batch_norm = BatchNormalization(**bn_config)\n",
    "            new_batch_norm.build(layer.input_shape)\n",
    "            new_moving_mean, new_moving_var = new_batch_norm.get_weights()[-2:]\n",
    "            # get rid of the new_batch_norm layer\n",
    "            del new_batch_norm\n",
    "            # get the trained gamma and beta from the current batch norm layer\n",
    "            trained_weights = layer.get_weights()\n",
    "            new_weights = []\n",
    "            # get gamma if exists\n",
    "            if bn_config['scale']:\n",
    "                new_weights.append(trained_weights.pop(0))\n",
    "            # get beta if exists\n",
    "            if bn_config['center']:\n",
    "                new_weights.append(trained_weights.pop(0))\n",
    "            new_weights += [new_moving_mean, new_moving_var]\n",
    "            # set weights to trained gamma and beta, reinitialized mean and variance\n",
    "            layer.set_weights(new_weights)\n",
    "\n",
    "    def _restore_batch_norm(self):\n",
    "\n",
    "        for layer, momentum in zip(self.batch_norm_layers, self.batch_norm_momentums):\n",
    "            layer.momentum = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "EPOCHS = 250\n",
    "NNBATCHSIZE = 16\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 321\n",
    "LR = 0.001\n",
    "SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train = pd.read_csv('../input/1-remove-drift-ac/train_feat_100k.csv.gz', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
    "    test  = pd.read_csv('../input/1-remove-drift-ac/test_feat_100k.csv.gz', dtype={'time': np.float32, 'signal': np.float32})\n",
    "    sub  = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n",
    "    return train, test, sub\n",
    "\n",
    "# create batches of 4000 observations\n",
    "def batching(df, batch_size):\n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "\n",
    "def normalize(train,test,features):\n",
    "    scal = preprocessing.StandardScaler()\n",
    "    scal.fit(train[features])\n",
    "    train[features] = scal.transform(train[features])\n",
    "    test[features] = scal.transform(test[features])\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame,\n",
    "                     verbose: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "\n",
    "                if (c_min > np.iinfo(np.int32).min\n",
    "                      and c_max < np.iinfo(np.int32).max):\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif (c_min > np.iinfo(np.int64).min\n",
    "                      and c_max < np.iinfo(np.int64).max):\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (c_min > np.finfo(np.float16).min\n",
    "                        and c_max < np.finfo(np.float16).max):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (c_min > np.finfo(np.float32).min\n",
    "                      and c_max < np.finfo(np.float32).max):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    reduction = (start_mem - end_mem) / start_mem\n",
    "\n",
    "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def Classifier(shape_):\n",
    "    \n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same', \n",
    "                              activation = 'tanh', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same',\n",
    "                              activation = 'sigmoid', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters = filters,\n",
    "                       kernel_size = 1,\n",
    "                       padding = 'same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "    \n",
    "    inp = Input(shape = (shape_))\n",
    "    \n",
    "    x = wave_block(inp, 16, 3, 12)\n",
    "    x1 = wave_block(x, 32, 3, 8)\n",
    "    x2 = wave_block(x1, 64, 3, 4)\n",
    "    x3 = wave_block(x2, 128, 3, 1)\n",
    "    \n",
    "    \n",
    "    out = Dense(11, activation = 'softmax', name = 'out')(x3)\n",
    "    \n",
    "    model = models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    opt = Adam(lr = LR)\n",
    "    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# function that decrease the learning as epochs increase (i also change this part of the code)\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 30:\n",
    "        lr = LR\n",
    "    elif epoch < 40:\n",
    "        lr = LR / 3\n",
    "    elif epoch < 50:\n",
    "        lr = LR / 5\n",
    "    elif epoch < 60:\n",
    "        lr = LR / 7\n",
    "    elif epoch < 70:\n",
    "        lr = LR / 9\n",
    "    elif epoch < 80:\n",
    "        lr = LR / 11\n",
    "    elif epoch < 90:\n",
    "        lr = LR / 13\n",
    "    elif epoch < 120:\n",
    "        lr = LR / 15\n",
    "    else:\n",
    "        lr = LR / 17\n",
    "    return lr\n",
    "\n",
    "# class to get macro f1 score. This is not entirely necessary but it's fun to check f1 score of each epoch (be carefull, if you use this function early stopping callback will not work)\n",
    "class MacroF1(Callback):\n",
    "    def __init__(self, model, inputs, targets):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n",
    "        score = f1_score(self.targets, pred, average = 'macro')\n",
    "        print(f'F1 Macro Score: {score:.5f}')\n",
    "        \n",
    "\n",
    "start_epoch = 200\n",
    "\n",
    "# define swa callback\n",
    "swa = SWA(start_epoch=start_epoch, \n",
    "          lr_schedule='manual',\n",
    "          swa_lr=0.001, \n",
    "          verbose=1)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# main function to perfrom groupkfold cross validation (we have 1000 vectores of 4000 rows and 8 features (columns)). Going to make 5 groups with this subgroups.\n",
    "def run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size):\n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "      \n",
    "    oof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n",
    "    preds_ = np.zeros((len(test), 11))\n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])    \n",
    "        new_splits.append(new_split)\n",
    "    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n",
    "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
    "\n",
    "    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
    "    target_cols = ['target_'+str(i) for i in range(11)]\n",
    "    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values.astype(np.float32))))\n",
    "    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n",
    "        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n",
    "        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n",
    "        print(f'Our training dataset shape is {train_x.shape}')\n",
    "        print(f'Our validation dataset shape is {valid_x.shape}')\n",
    "\n",
    "        shape_ = (None, train_x.shape[2]) # input is going to be the number of feature we are using (dimension 2 of 0, 1, 2)\n",
    "        model = Classifier(shape_)\n",
    "        # using our lr_schedule function\n",
    "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(train_x,train_y,\n",
    "                  epochs = nn_epochs,\n",
    "                  callbacks = [swa,cb_lr_schedule],# MacroF1(model, valid_x, valid_y)], # adding custom evaluation metric for each epoch\n",
    "                  batch_size = nn_batch_size,verbose = 2,\n",
    "                  validation_data = (valid_x,valid_y))\n",
    "        gc.collect()\n",
    "        preds_f = model.predict(valid_x)\n",
    "        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro') # need to get the class with the biggest probability\n",
    "        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
    "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        oof_[val_orig_idx,:] += preds_f\n",
    "        te_preds = model.predict(test)\n",
    "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
    "        preds_ += te_preds / SPLITS\n",
    "    \n",
    "        \n",
    "    # calculate the oof macro f1_score\n",
    "    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n",
    "    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
    "    \n",
    "    np.save(f'./oof.{f1_score_:1.5f}.npy',oof_)\n",
    "    np.save(f'./preds.{f1_score_:1.5f}.npy',preds_)\n",
    "    \n",
    "    \n",
    "    sample_submission['open_channels'] = np.argmax(preds_, axis = 1).astype(int)\n",
    "    sample_submission.to_csv(f'./submission_wavenet.{f1_score_:1.5f}.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Reading Data Started...')\n",
    "train, test, sample_submission = read_data()\n",
    "train=train[0:5004000]\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 534.48 MB (72.9 % reduction)\n",
      "Mem. usage decreased to 205.99 MB (73.7 % reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=['n','pt', 'm2am_50', 'm2am_100', 'm2am_500', 'P3', 'm2am_1000', 'signal', 'm2am_10', 'm2am_10000',\n",
    "     'P0', 'P1', 'P4', 'sdam_10000', 'P2', 'm2am_5', 'P6', 'P9', 'P5', 'P8', 'P7', 'P10', 'sdm_5000',\n",
    "     'sdm_500', 'sdam_1000', 'lma_2', 'lmi_3', 'lmi_2', 'sdam_5000', 'm2am_5000', 'm2_2', 'lmi_5', \n",
    "     'sdm_10000', 'lmed_2', 'lead_1', 'lma_3', 'm2am_4', 'pt1', 'lmi_4', 'leadm_1', 'vp', 'lmim_2',\n",
    "     'lma_50', 'lmi_10', 'lma_10', 'lmim_10', 'lmed_3', 'sdm_10', 'vp1', 'm2m_10000', 'sdam_50']\n",
    "\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "\n",
    "train, test = normalize(train,test,['signal'])\n",
    "gc.collect()\n",
    "\n",
    "test=test[features]\n",
    "features.append('open_channels')\n",
    "train=train[features]\n",
    "\n",
    "train = batching(train, batch_size = GROUP_BATCH_SIZE)\n",
    "test  = batching(test , batch_size = GROUP_BATCH_SIZE)\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "\n",
    "features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Wavenet model with 5 folds of GroupKFold Started...\n",
      "Our training dataset shape is (1000, 4000, 51)\n",
      "Our validation dataset shape is (251, 4000, 51)\n",
      "Train on 1000 samples, validate on 251 samples\n",
      "Epoch 1/250\n",
      "1000/1000 - 22s - loss: 0.5285 - accuracy: 0.8273 - val_loss: 0.1663 - val_accuracy: 0.9451\n",
      "Epoch 2/250\n",
      "1000/1000 - 8s - loss: 0.1263 - accuracy: 0.9566 - val_loss: 0.1024 - val_accuracy: 0.9639\n",
      "Epoch 3/250\n",
      "1000/1000 - 8s - loss: 0.1406 - accuracy: 0.9446 - val_loss: 0.1077 - val_accuracy: 0.9640\n",
      "Epoch 4/250\n",
      "1000/1000 - 8s - loss: 0.0940 - accuracy: 0.9663 - val_loss: 0.0859 - val_accuracy: 0.9686\n",
      "Epoch 5/250\n",
      "1000/1000 - 7s - loss: 0.0864 - accuracy: 0.9681 - val_loss: 0.0850 - val_accuracy: 0.9680\n",
      "Epoch 6/250\n",
      "1000/1000 - 7s - loss: 0.0834 - accuracy: 0.9688 - val_loss: 0.0811 - val_accuracy: 0.9694\n",
      "Epoch 7/250\n",
      "1000/1000 - 8s - loss: 0.0821 - accuracy: 0.9690 - val_loss: 0.0808 - val_accuracy: 0.9693\n",
      "Epoch 8/250\n",
      "1000/1000 - 7s - loss: 0.0829 - accuracy: 0.9687 - val_loss: 0.0861 - val_accuracy: 0.9672\n",
      "Epoch 9/250\n",
      "1000/1000 - 7s - loss: 0.0832 - accuracy: 0.9685 - val_loss: 0.0796 - val_accuracy: 0.9697\n",
      "Epoch 10/250\n",
      "1000/1000 - 7s - loss: 0.0812 - accuracy: 0.9691 - val_loss: 0.0810 - val_accuracy: 0.9691\n",
      "Epoch 11/250\n",
      "1000/1000 - 8s - loss: 0.0885 - accuracy: 0.9665 - val_loss: 0.0842 - val_accuracy: 0.9683\n",
      "Epoch 12/250\n",
      "1000/1000 - 8s - loss: 0.0974 - accuracy: 0.9632 - val_loss: 0.0811 - val_accuracy: 0.9692\n",
      "Epoch 13/250\n",
      "1000/1000 - 7s - loss: 0.0814 - accuracy: 0.9691 - val_loss: 0.0790 - val_accuracy: 0.9699\n",
      "Epoch 14/250\n",
      "1000/1000 - 7s - loss: 0.0789 - accuracy: 0.9699 - val_loss: 0.0781 - val_accuracy: 0.9701\n",
      "Epoch 15/250\n",
      "1000/1000 - 8s - loss: 0.0832 - accuracy: 0.9682 - val_loss: 0.1256 - val_accuracy: 0.9530\n",
      "Epoch 16/250\n",
      "1000/1000 - 8s - loss: 0.0924 - accuracy: 0.9653 - val_loss: 0.0799 - val_accuracy: 0.9695\n",
      "Epoch 17/250\n",
      "1000/1000 - 8s - loss: 0.0800 - accuracy: 0.9695 - val_loss: 0.0777 - val_accuracy: 0.9702\n",
      "Epoch 18/250\n",
      "1000/1000 - 7s - loss: 0.0785 - accuracy: 0.9699 - val_loss: 0.0781 - val_accuracy: 0.9700\n",
      "Epoch 19/250\n",
      "1000/1000 - 8s - loss: 0.0782 - accuracy: 0.9700 - val_loss: 0.0771 - val_accuracy: 0.9704\n",
      "Epoch 20/250\n",
      "1000/1000 - 8s - loss: 0.0780 - accuracy: 0.9701 - val_loss: 0.0774 - val_accuracy: 0.9704\n",
      "Epoch 21/250\n",
      "1000/1000 - 8s - loss: 0.0807 - accuracy: 0.9691 - val_loss: 0.0775 - val_accuracy: 0.9701\n",
      "Epoch 22/250\n",
      "1000/1000 - 7s - loss: 0.0780 - accuracy: 0.9701 - val_loss: 0.0773 - val_accuracy: 0.9704\n",
      "Epoch 23/250\n",
      "1000/1000 - 7s - loss: 0.0777 - accuracy: 0.9701 - val_loss: 0.0767 - val_accuracy: 0.9706\n",
      "Epoch 24/250\n",
      "1000/1000 - 8s - loss: 0.0787 - accuracy: 0.9698 - val_loss: 0.0775 - val_accuracy: 0.9702\n",
      "Epoch 25/250\n",
      "1000/1000 - 8s - loss: 0.0779 - accuracy: 0.9701 - val_loss: 0.0779 - val_accuracy: 0.9701\n",
      "Epoch 26/250\n",
      "1000/1000 - 7s - loss: 0.0772 - accuracy: 0.9703 - val_loss: 0.0767 - val_accuracy: 0.9705\n",
      "Epoch 27/250\n",
      "1000/1000 - 8s - loss: 0.0804 - accuracy: 0.9691 - val_loss: 0.0779 - val_accuracy: 0.9700\n",
      "Epoch 28/250\n",
      "1000/1000 - 8s - loss: 0.0785 - accuracy: 0.9698 - val_loss: 0.0775 - val_accuracy: 0.9702\n",
      "Epoch 29/250\n",
      "1000/1000 - 8s - loss: 0.0771 - accuracy: 0.9703 - val_loss: 0.0765 - val_accuracy: 0.9705\n",
      "Epoch 30/250\n",
      "1000/1000 - 7s - loss: 0.0776 - accuracy: 0.9701 - val_loss: 0.0775 - val_accuracy: 0.9700\n",
      "Epoch 31/250\n",
      "1000/1000 - 8s - loss: 0.0764 - accuracy: 0.9705 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 32/250\n",
      "1000/1000 - 7s - loss: 0.0761 - accuracy: 0.9707 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 33/250\n",
      "1000/1000 - 7s - loss: 0.0761 - accuracy: 0.9706 - val_loss: 0.0760 - val_accuracy: 0.9707\n",
      "Epoch 34/250\n",
      "1000/1000 - 8s - loss: 0.0760 - accuracy: 0.9707 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 35/250\n",
      "1000/1000 - 8s - loss: 0.0759 - accuracy: 0.9707 - val_loss: 0.0757 - val_accuracy: 0.9708\n",
      "Epoch 36/250\n",
      "1000/1000 - 8s - loss: 0.0759 - accuracy: 0.9707 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 37/250\n",
      "1000/1000 - 8s - loss: 0.0760 - accuracy: 0.9706 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 38/250\n",
      "1000/1000 - 8s - loss: 0.0759 - accuracy: 0.9706 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 39/250\n",
      "1000/1000 - 7s - loss: 0.0758 - accuracy: 0.9707 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 40/250\n",
      "1000/1000 - 7s - loss: 0.0758 - accuracy: 0.9707 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 41/250\n",
      "1000/1000 - 7s - loss: 0.0757 - accuracy: 0.9707 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 42/250\n",
      "1000/1000 - 7s - loss: 0.0757 - accuracy: 0.9707 - val_loss: 0.0754 - val_accuracy: 0.9708\n",
      "Epoch 43/250\n",
      "1000/1000 - 8s - loss: 0.0756 - accuracy: 0.9708 - val_loss: 0.0753 - val_accuracy: 0.9709\n",
      "Epoch 44/250\n",
      "1000/1000 - 8s - loss: 0.0756 - accuracy: 0.9708 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 45/250\n",
      "1000/1000 - 7s - loss: 0.0756 - accuracy: 0.9707 - val_loss: 0.0753 - val_accuracy: 0.9710\n",
      "Epoch 46/250\n",
      "1000/1000 - 7s - loss: 0.0760 - accuracy: 0.9707 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 47/250\n",
      "1000/1000 - 8s - loss: 0.0755 - accuracy: 0.9708 - val_loss: 0.0752 - val_accuracy: 0.9709\n",
      "Epoch 48/250\n",
      "1000/1000 - 7s - loss: 0.0755 - accuracy: 0.9708 - val_loss: 0.0754 - val_accuracy: 0.9708\n",
      "Epoch 49/250\n",
      "1000/1000 - 7s - loss: 0.0755 - accuracy: 0.9707 - val_loss: 0.0752 - val_accuracy: 0.9709\n",
      "Epoch 50/250\n",
      "1000/1000 - 7s - loss: 0.0757 - accuracy: 0.9707 - val_loss: 0.0761 - val_accuracy: 0.9706\n",
      "Epoch 51/250\n",
      "1000/1000 - 8s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 52/250\n",
      "1000/1000 - 8s - loss: 0.0753 - accuracy: 0.9708 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 53/250\n",
      "1000/1000 - 7s - loss: 0.0754 - accuracy: 0.9709 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 54/250\n",
      "1000/1000 - 7s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 55/250\n",
      "1000/1000 - 8s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0751 - val_accuracy: 0.9710\n",
      "Epoch 56/250\n",
      "1000/1000 - 8s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 57/250\n",
      "1000/1000 - 7s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0753 - val_accuracy: 0.9709\n",
      "Epoch 58/250\n",
      "1000/1000 - 7s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 59/250\n",
      "1000/1000 - 8s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 60/250\n",
      "1000/1000 - 8s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0750 - val_accuracy: 0.9709\n",
      "Epoch 61/250\n",
      "1000/1000 - 7s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 62/250\n",
      "1000/1000 - 7s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0750 - val_accuracy: 0.9709\n",
      "Epoch 63/250\n",
      "1000/1000 - 7s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0750 - val_accuracy: 0.9709\n",
      "Epoch 64/250\n",
      "1000/1000 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 65/250\n",
      "1000/1000 - 7s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 66/250\n",
      "1000/1000 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0750 - val_accuracy: 0.9709\n",
      "Epoch 67/250\n",
      "1000/1000 - 8s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 68/250\n",
      "1000/1000 - 8s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 69/250\n",
      "1000/1000 - 8s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 70/250\n",
      "1000/1000 - 7s - loss: 0.0753 - accuracy: 0.9708 - val_loss: 0.0750 - val_accuracy: 0.9709\n",
      "Epoch 71/250\n",
      "1000/1000 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0749 - val_accuracy: 0.9709\n",
      "Epoch 72/250\n",
      "1000/1000 - 7s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 73/250\n",
      "1000/1000 - 7s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 74/250\n",
      "1000/1000 - 8s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 75/250\n",
      "1000/1000 - 8s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 76/250\n",
      "1000/1000 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 77/250\n",
      "1000/1000 - 7s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 78/250\n",
      "1000/1000 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 79/250\n",
      "1000/1000 - 7s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 80/250\n",
      "1000/1000 - 7s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 81/250\n",
      "1000/1000 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 82/250\n",
      "1000/1000 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 83/250\n",
      "1000/1000 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 84/250\n",
      "1000/1000 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 85/250\n",
      "1000/1000 - 7s - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 86/250\n",
      "1000/1000 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 87/250\n",
      "1000/1000 - 8s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 88/250\n",
      "1000/1000 - 7s - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.0749 - val_accuracy: 0.9711\n",
      "Epoch 89/250\n",
      "1000/1000 - 7s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 90/250\n",
      "1000/1000 - 8s - loss: 0.0747 - accuracy: 0.9710 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 91/250\n",
      "1000/1000 - 8s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 92/250\n",
      "1000/1000 - 7s - loss: 0.0747 - accuracy: 0.9710 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 93/250\n",
      "1000/1000 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9710\n",
      "Epoch 94/250\n",
      "1000/1000 - 7s - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 95/250\n",
      "1000/1000 - 7s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 96/250\n",
      "1000/1000 - 8s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 97/250\n",
      "1000/1000 - 7s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 98/250\n",
      "1000/1000 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 99/250\n",
      "1000/1000 - 8s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 100/250\n",
      "1000/1000 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 101/250\n",
      "1000/1000 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 102/250\n",
      "1000/1000 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 103/250\n",
      "1000/1000 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 104/250\n",
      "1000/1000 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 105/250\n",
      "1000/1000 - 8s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 106/250\n",
      "1000/1000 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 107/250\n",
      "1000/1000 - 8s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 108/250\n",
      "1000/1000 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 109/250\n",
      "1000/1000 - 8s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 110/250\n",
      "1000/1000 - 8s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 111/250\n",
      "1000/1000 - 7s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 112/250\n",
      "1000/1000 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 113/250\n",
      "1000/1000 - 7s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 114/250\n",
      "1000/1000 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 115/250\n",
      "1000/1000 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0747 - val_accuracy: 0.9710\n",
      "Epoch 116/250\n",
      "1000/1000 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 117/250\n",
      "1000/1000 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 118/250\n",
      "1000/1000 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 119/250\n",
      "1000/1000 - 7s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 120/250\n",
      "1000/1000 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 121/250\n",
      "1000/1000 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0745 - val_accuracy: 0.9713\n",
      "Epoch 122/250\n",
      "1000/1000 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0744 - val_accuracy: 0.9713\n",
      "Epoch 123/250\n",
      "1000/1000 - 8s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 124/250\n",
      "1000/1000 - 8s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 125/250\n",
      "1000/1000 - 8s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 126/250\n",
      "1000/1000 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 127/250\n",
      "1000/1000 - 7s - loss: 0.0739 - accuracy: 0.9714 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 128/250\n",
      "1000/1000 - 8s - loss: 0.0739 - accuracy: 0.9714 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 129/250\n",
      "1000/1000 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 130/250\n",
      "1000/1000 - 7s - loss: 0.0737 - accuracy: 0.9714 - val_loss: 0.0741 - val_accuracy: 0.9714\n",
      "Epoch 131/250\n",
      "1000/1000 - 8s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0741 - val_accuracy: 0.9714\n",
      "Epoch 132/250\n",
      "1000/1000 - 8s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0741 - val_accuracy: 0.9714\n",
      "Epoch 133/250\n",
      "1000/1000 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 134/250\n",
      "1000/1000 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0740 - val_accuracy: 0.9715\n",
      "Epoch 135/250\n",
      "1000/1000 - 7s - loss: 0.0734 - accuracy: 0.9716 - val_loss: 0.0740 - val_accuracy: 0.9714\n",
      "Epoch 136/250\n",
      "1000/1000 - 7s - loss: 0.0734 - accuracy: 0.9716 - val_loss: 0.0738 - val_accuracy: 0.9715\n",
      "Epoch 137/250\n",
      "1000/1000 - 7s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0738 - val_accuracy: 0.9715\n",
      "Epoch 138/250\n",
      "1000/1000 - 7s - loss: 0.0732 - accuracy: 0.9716 - val_loss: 0.0736 - val_accuracy: 0.9716\n",
      "Epoch 139/250\n",
      "1000/1000 - 8s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0738 - val_accuracy: 0.9716\n",
      "Epoch 140/250\n",
      "1000/1000 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0737 - val_accuracy: 0.9716\n",
      "Epoch 141/250\n",
      "1000/1000 - 8s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0736 - val_accuracy: 0.9716\n",
      "Epoch 142/250\n",
      "1000/1000 - 7s - loss: 0.0728 - accuracy: 0.9719 - val_loss: 0.0734 - val_accuracy: 0.9716\n",
      "Epoch 143/250\n",
      "1000/1000 - 7s - loss: 0.0728 - accuracy: 0.9719 - val_loss: 0.0733 - val_accuracy: 0.9716\n",
      "Epoch 144/250\n",
      "1000/1000 - 8s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0734 - val_accuracy: 0.9716\n",
      "Epoch 145/250\n",
      "1000/1000 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0732 - val_accuracy: 0.9717\n",
      "Epoch 146/250\n",
      "1000/1000 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0733 - val_accuracy: 0.9716\n",
      "Epoch 147/250\n",
      "1000/1000 - 8s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0733 - val_accuracy: 0.9717\n",
      "Epoch 148/250\n",
      "1000/1000 - 8s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0732 - val_accuracy: 0.9717\n",
      "Epoch 149/250\n",
      "1000/1000 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0731 - val_accuracy: 0.9718\n",
      "Epoch 150/250\n",
      "1000/1000 - 7s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 151/250\n",
      "1000/1000 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0732 - val_accuracy: 0.9717\n",
      "Epoch 152/250\n",
      "1000/1000 - 7s - loss: 0.0728 - accuracy: 0.9719 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 153/250\n",
      "1000/1000 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 154/250\n",
      "1000/1000 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 155/250\n",
      "1000/1000 - 8s - loss: 0.0723 - accuracy: 0.9721 - val_loss: 0.0731 - val_accuracy: 0.9718\n",
      "Epoch 156/250\n",
      "1000/1000 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 157/250\n",
      "1000/1000 - 7s - loss: 0.0723 - accuracy: 0.9721 - val_loss: 0.0733 - val_accuracy: 0.9717\n",
      "Epoch 158/250\n",
      "1000/1000 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 159/250\n",
      "1000/1000 - 8s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 160/250\n",
      "1000/1000 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0730 - val_accuracy: 0.9719\n",
      "Epoch 161/250\n",
      "1000/1000 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0730 - val_accuracy: 0.9719\n",
      "Epoch 162/250\n",
      "1000/1000 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 163/250\n",
      "1000/1000 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 164/250\n",
      "1000/1000 - 8s - loss: 0.0723 - accuracy: 0.9721 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 165/250\n",
      "1000/1000 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0729 - val_accuracy: 0.9719\n",
      "Epoch 166/250\n",
      "1000/1000 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 167/250\n",
      "1000/1000 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 168/250\n",
      "1000/1000 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0729 - val_accuracy: 0.9719\n",
      "Epoch 169/250\n",
      "1000/1000 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 170/250\n",
      "1000/1000 - 8s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 171/250\n",
      "1000/1000 - 8s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 172/250\n",
      "1000/1000 - 8s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 173/250\n",
      "1000/1000 - 8s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 174/250\n",
      "1000/1000 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 175/250\n",
      "1000/1000 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0729 - val_accuracy: 0.9719\n",
      "Epoch 176/250\n",
      "1000/1000 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 177/250\n",
      "1000/1000 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 178/250\n",
      "1000/1000 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0730 - val_accuracy: 0.9719\n",
      "Epoch 179/250\n",
      "1000/1000 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0733 - val_accuracy: 0.9716\n",
      "Epoch 180/250\n",
      "1000/1000 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 181/250\n",
      "1000/1000 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 182/250\n",
      "1000/1000 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 183/250\n",
      "1000/1000 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 184/250\n",
      "1000/1000 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 185/250\n",
      "1000/1000 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 186/250\n",
      "1000/1000 - 8s - loss: 0.0718 - accuracy: 0.9723 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 187/250\n",
      "1000/1000 - 8s - loss: 0.0718 - accuracy: 0.9723 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 188/250\n",
      "1000/1000 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 189/250\n",
      "1000/1000 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 190/250\n",
      "1000/1000 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 191/250\n",
      "1000/1000 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 192/250\n",
      "1000/1000 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0726 - val_accuracy: 0.9719\n",
      "Epoch 193/250\n",
      "1000/1000 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 194/250\n",
      "1000/1000 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0726 - val_accuracy: 0.9719\n",
      "Epoch 195/250\n",
      "1000/1000 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 196/250\n",
      "1000/1000 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 197/250\n",
      "1000/1000 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 198/250\n",
      "1000/1000 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 199/250\n",
      "1000/1000 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00200: starting stochastic weight averaging\n",
      "Epoch 200/250\n",
      "1000/1000 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 201/250\n",
      "1000/1000 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 202/250\n",
      "1000/1000 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0726 - val_accuracy: 0.9719\n",
      "Epoch 203/250\n",
      "1000/1000 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 204/250\n",
      "1000/1000 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 205/250\n",
      "1000/1000 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 206/250\n",
      "1000/1000 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 207/250\n",
      "1000/1000 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 208/250\n",
      "1000/1000 - 8s - loss: 0.0715 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 209/250\n",
      "1000/1000 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 210/250\n",
      "1000/1000 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0728 - val_accuracy: 0.9718\n",
      "Epoch 211/250\n",
      "1000/1000 - 8s - loss: 0.0715 - accuracy: 0.9724 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 212/250\n",
      "1000/1000 - 8s - loss: 0.0715 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 213/250\n",
      "1000/1000 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0726 - val_accuracy: 0.9719\n",
      "Epoch 214/250\n",
      "1000/1000 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 215/250\n",
      "1000/1000 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0725 - val_accuracy: 0.9720\n",
      "Epoch 216/250\n",
      "1000/1000 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0725 - val_accuracy: 0.9720\n",
      "Epoch 217/250\n",
      "1000/1000 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 218/250\n",
      "1000/1000 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 219/250\n",
      "1000/1000 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0728 - val_accuracy: 0.9720\n",
      "Epoch 220/250\n",
      "1000/1000 - 8s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 221/250\n",
      "1000/1000 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0729 - val_accuracy: 0.9719\n",
      "Epoch 222/250\n",
      "1000/1000 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 223/250\n",
      "1000/1000 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 224/250\n",
      "1000/1000 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 225/250\n",
      "1000/1000 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 226/250\n",
      "1000/1000 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 227/250\n",
      "1000/1000 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 228/250\n",
      "1000/1000 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 229/250\n",
      "1000/1000 - 7s - loss: 0.0712 - accuracy: 0.9725 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 230/250\n",
      "1000/1000 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 231/250\n",
      "1000/1000 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 232/250\n",
      "1000/1000 - 7s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 233/250\n",
      "1000/1000 - 7s - loss: 0.0712 - accuracy: 0.9725 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 234/250\n",
      "1000/1000 - 8s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 235/250\n",
      "1000/1000 - 8s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 236/250\n",
      "1000/1000 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 237/250\n",
      "1000/1000 - 8s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 238/250\n",
      "1000/1000 - 7s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 239/250\n",
      "1000/1000 - 8s - loss: 0.0712 - accuracy: 0.9725 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 240/250\n",
      "1000/1000 - 8s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 241/250\n",
      "1000/1000 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 242/250\n",
      "1000/1000 - 8s - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.0728 - val_accuracy: 0.9720\n",
      "Epoch 243/250\n",
      "1000/1000 - 8s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 244/250\n",
      "1000/1000 - 8s - loss: 0.0712 - accuracy: 0.9725 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 245/250\n",
      "1000/1000 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 246/250\n",
      "1000/1000 - 8s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 247/250\n",
      "1000/1000 - 8s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 248/250\n",
      "1000/1000 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 249/250\n",
      "1000/1000 - 8s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 250/250\n",
      "1000/1000 - 8s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0728 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00251: final model weights set to stochastic weight average\n",
      "Training fold 1 completed. macro f1 score : 0.94377\n",
      "Our training dataset shape is (1001, 4000, 51)\n",
      "Our validation dataset shape is (250, 4000, 51)\n",
      "Train on 1001 samples, validate on 250 samples\n",
      "Epoch 1/250\n",
      "1001/1001 - 17s - loss: 0.6439 - accuracy: 0.8285 - val_loss: 0.1752 - val_accuracy: 0.9474\n",
      "Epoch 2/250\n",
      "1001/1001 - 7s - loss: 0.1321 - accuracy: 0.9574 - val_loss: 0.1046 - val_accuracy: 0.9624\n",
      "Epoch 3/250\n",
      "1001/1001 - 7s - loss: 0.0999 - accuracy: 0.9642 - val_loss: 0.0897 - val_accuracy: 0.9675\n",
      "Epoch 4/250\n",
      "1001/1001 - 7s - loss: 0.0872 - accuracy: 0.9680 - val_loss: 0.0828 - val_accuracy: 0.9691\n",
      "Epoch 5/250\n",
      "1001/1001 - 8s - loss: 0.0900 - accuracy: 0.9663 - val_loss: 0.1110 - val_accuracy: 0.9593\n",
      "Epoch 6/250\n",
      "1001/1001 - 8s - loss: 0.0878 - accuracy: 0.9672 - val_loss: 0.0811 - val_accuracy: 0.9695\n",
      "Epoch 7/250\n",
      "1001/1001 - 7s - loss: 0.0827 - accuracy: 0.9687 - val_loss: 0.1400 - val_accuracy: 0.9479\n",
      "Epoch 8/250\n",
      "1001/1001 - 7s - loss: 0.1162 - accuracy: 0.9570 - val_loss: 0.0866 - val_accuracy: 0.9675\n",
      "Epoch 9/250\n",
      "1001/1001 - 8s - loss: 0.0829 - accuracy: 0.9688 - val_loss: 0.0787 - val_accuracy: 0.9700\n",
      "Epoch 10/250\n",
      "1001/1001 - 7s - loss: 0.0798 - accuracy: 0.9696 - val_loss: 0.0788 - val_accuracy: 0.9699\n",
      "Epoch 11/250\n",
      "1001/1001 - 7s - loss: 0.0796 - accuracy: 0.9696 - val_loss: 0.0783 - val_accuracy: 0.9702\n",
      "Epoch 12/250\n",
      "1001/1001 - 7s - loss: 0.0789 - accuracy: 0.9698 - val_loss: 0.0777 - val_accuracy: 0.9702\n",
      "Epoch 13/250\n",
      "1001/1001 - 8s - loss: 0.0795 - accuracy: 0.9695 - val_loss: 0.0776 - val_accuracy: 0.9702\n",
      "Epoch 14/250\n",
      "1001/1001 - 8s - loss: 0.0789 - accuracy: 0.9697 - val_loss: 0.0775 - val_accuracy: 0.9703\n",
      "Epoch 15/250\n",
      "1001/1001 - 8s - loss: 0.0877 - accuracy: 0.9666 - val_loss: 0.1000 - val_accuracy: 0.9621\n",
      "Epoch 16/250\n",
      "1001/1001 - 7s - loss: 0.0924 - accuracy: 0.9654 - val_loss: 0.0793 - val_accuracy: 0.9698\n",
      "Epoch 17/250\n",
      "1001/1001 - 7s - loss: 0.0793 - accuracy: 0.9697 - val_loss: 0.0764 - val_accuracy: 0.9706\n",
      "Epoch 18/250\n",
      "1001/1001 - 7s - loss: 0.0779 - accuracy: 0.9700 - val_loss: 0.0771 - val_accuracy: 0.9702\n",
      "Epoch 19/250\n",
      "1001/1001 - 7s - loss: 0.0788 - accuracy: 0.9697 - val_loss: 0.0760 - val_accuracy: 0.9707\n",
      "Epoch 20/250\n",
      "1001/1001 - 7s - loss: 0.0780 - accuracy: 0.9700 - val_loss: 0.0784 - val_accuracy: 0.9700\n",
      "Epoch 21/250\n",
      "1001/1001 - 8s - loss: 0.0831 - accuracy: 0.9682 - val_loss: 0.0793 - val_accuracy: 0.9697\n",
      "Epoch 22/250\n",
      "1001/1001 - 8s - loss: 0.0779 - accuracy: 0.9700 - val_loss: 0.0758 - val_accuracy: 0.9707\n",
      "Epoch 23/250\n",
      "1001/1001 - 7s - loss: 0.0771 - accuracy: 0.9702 - val_loss: 0.0791 - val_accuracy: 0.9698\n",
      "Epoch 24/250\n",
      "1001/1001 - 7s - loss: 0.0775 - accuracy: 0.9702 - val_loss: 0.0760 - val_accuracy: 0.9707\n",
      "Epoch 25/250\n",
      "1001/1001 - 7s - loss: 0.0769 - accuracy: 0.9703 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 26/250\n",
      "1001/1001 - 7s - loss: 0.0772 - accuracy: 0.9702 - val_loss: 0.0786 - val_accuracy: 0.9700\n",
      "Epoch 27/250\n",
      "1001/1001 - 7s - loss: 0.0776 - accuracy: 0.9700 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 28/250\n",
      "1001/1001 - 7s - loss: 0.0769 - accuracy: 0.9703 - val_loss: 0.0804 - val_accuracy: 0.9690\n",
      "Epoch 29/250\n",
      "1001/1001 - 8s - loss: 0.0808 - accuracy: 0.9689 - val_loss: 0.0799 - val_accuracy: 0.9694\n",
      "Epoch 30/250\n",
      "1001/1001 - 8s - loss: 0.0794 - accuracy: 0.9694 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 31/250\n",
      "1001/1001 - 7s - loss: 0.0761 - accuracy: 0.9706 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 32/250\n",
      "1001/1001 - 7s - loss: 0.0759 - accuracy: 0.9706 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 33/250\n",
      "1001/1001 - 7s - loss: 0.0759 - accuracy: 0.9706 - val_loss: 0.0751 - val_accuracy: 0.9710\n",
      "Epoch 34/250\n",
      "1001/1001 - 7s - loss: 0.0758 - accuracy: 0.9706 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 35/250\n",
      "1001/1001 - 7s - loss: 0.0758 - accuracy: 0.9706 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 36/250\n",
      "1001/1001 - 7s - loss: 0.0758 - accuracy: 0.9706 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 37/250\n",
      "1001/1001 - 8s - loss: 0.0758 - accuracy: 0.9706 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 38/250\n",
      "1001/1001 - 8s - loss: 0.0757 - accuracy: 0.9707 - val_loss: 0.0745 - val_accuracy: 0.9711\n",
      "Epoch 39/250\n",
      "1001/1001 - 7s - loss: 0.0757 - accuracy: 0.9706 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 40/250\n",
      "1001/1001 - 7s - loss: 0.0756 - accuracy: 0.9707 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 41/250\n",
      "1001/1001 - 7s - loss: 0.0755 - accuracy: 0.9707 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 42/250\n",
      "1001/1001 - 7s - loss: 0.0755 - accuracy: 0.9707 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 43/250\n",
      "1001/1001 - 7s - loss: 0.0755 - accuracy: 0.9707 - val_loss: 0.0744 - val_accuracy: 0.9711\n",
      "Epoch 44/250\n",
      "1001/1001 - 7s - loss: 0.0754 - accuracy: 0.9707 - val_loss: 0.0744 - val_accuracy: 0.9711\n",
      "Epoch 45/250\n",
      "1001/1001 - 8s - loss: 0.0755 - accuracy: 0.9707 - val_loss: 0.0745 - val_accuracy: 0.9711\n",
      "Epoch 46/250\n",
      "1001/1001 - 7s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 47/250\n",
      "1001/1001 - 7s - loss: 0.0755 - accuracy: 0.9707 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 48/250\n",
      "1001/1001 - 7s - loss: 0.0754 - accuracy: 0.9707 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 49/250\n",
      "1001/1001 - 7s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0747 - val_accuracy: 0.9710\n",
      "Epoch 50/250\n",
      "1001/1001 - 8s - loss: 0.0755 - accuracy: 0.9707 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 51/250\n",
      "1001/1001 - 7s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 52/250\n",
      "1001/1001 - 7s - loss: 0.0753 - accuracy: 0.9708 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 53/250\n",
      "1001/1001 - 8s - loss: 0.0752 - accuracy: 0.9708 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 54/250\n",
      "1001/1001 - 8s - loss: 0.0752 - accuracy: 0.9708 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 55/250\n",
      "1001/1001 - 7s - loss: 0.0753 - accuracy: 0.9708 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 56/250\n",
      "1001/1001 - 7s - loss: 0.0753 - accuracy: 0.9708 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 57/250\n",
      "1001/1001 - 7s - loss: 0.0752 - accuracy: 0.9708 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 58/250\n",
      "1001/1001 - 7s - loss: 0.0752 - accuracy: 0.9708 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 59/250\n",
      "1001/1001 - 7s - loss: 0.0752 - accuracy: 0.9708 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 60/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9708 - val_loss: 0.0741 - val_accuracy: 0.9712\n",
      "Epoch 61/250\n",
      "1001/1001 - 8s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 62/250\n",
      "1001/1001 - 8s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 63/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 64/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 65/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9712\n",
      "Epoch 66/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 67/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 68/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 69/250\n",
      "1001/1001 - 8s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 70/250\n",
      "1001/1001 - 8s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 71/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 72/250\n",
      "1001/1001 - 7s - loss: 0.0749 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 73/250\n",
      "1001/1001 - 7s - loss: 0.0749 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 74/250\n",
      "1001/1001 - 7s - loss: 0.0749 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9712\n",
      "Epoch 75/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 76/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0740 - val_accuracy: 0.9713\n",
      "Epoch 77/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 78/250\n",
      "1001/1001 - 8s - loss: 0.0750 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 79/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0740 - val_accuracy: 0.9713\n",
      "Epoch 80/250\n",
      "1001/1001 - 7s - loss: 0.0749 - accuracy: 0.9709 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 81/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9709 - val_loss: 0.0741 - val_accuracy: 0.9712\n",
      "Epoch 82/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 83/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 84/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 85/250\n",
      "1001/1001 - 8s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 86/250\n",
      "1001/1001 - 8s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0740 - val_accuracy: 0.9713\n",
      "Epoch 87/250\n",
      "1001/1001 - 8s - loss: 0.0747 - accuracy: 0.9710 - val_loss: 0.0740 - val_accuracy: 0.9713\n",
      "Epoch 88/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9710 - val_loss: 0.0740 - val_accuracy: 0.9714\n",
      "Epoch 89/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9710 - val_loss: 0.0739 - val_accuracy: 0.9714\n",
      "Epoch 90/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0741 - val_accuracy: 0.9713\n",
      "Epoch 91/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9710 - val_loss: 0.0739 - val_accuracy: 0.9714\n",
      "Epoch 92/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9710 - val_loss: 0.0739 - val_accuracy: 0.9713\n",
      "Epoch 93/250\n",
      "1001/1001 - 8s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0739 - val_accuracy: 0.9713\n",
      "Epoch 94/250\n",
      "1001/1001 - 8s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0739 - val_accuracy: 0.9713\n",
      "Epoch 95/250\n",
      "1001/1001 - 8s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0739 - val_accuracy: 0.9713\n",
      "Epoch 96/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0738 - val_accuracy: 0.9714\n",
      "Epoch 97/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0738 - val_accuracy: 0.9714\n",
      "Epoch 98/250\n",
      "1001/1001 - 8s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0740 - val_accuracy: 0.9713\n",
      "Epoch 99/250\n",
      "1001/1001 - 8s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0738 - val_accuracy: 0.9715\n",
      "Epoch 100/250\n",
      "1001/1001 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0739 - val_accuracy: 0.9715\n",
      "Epoch 101/250\n",
      "1001/1001 - 8s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0737 - val_accuracy: 0.9715\n",
      "Epoch 102/250\n",
      "1001/1001 - 8s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0737 - val_accuracy: 0.9715\n",
      "Epoch 103/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 104/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0735 - val_accuracy: 0.9716\n",
      "Epoch 105/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0735 - val_accuracy: 0.9717\n",
      "Epoch 106/250\n",
      "1001/1001 - 7s - loss: 0.0737 - accuracy: 0.9715 - val_loss: 0.0733 - val_accuracy: 0.9717\n",
      "Epoch 107/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0732 - val_accuracy: 0.9717\n",
      "Epoch 108/250\n",
      "1001/1001 - 8s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0732 - val_accuracy: 0.9717\n",
      "Epoch 109/250\n",
      "1001/1001 - 8s - loss: 0.0737 - accuracy: 0.9715 - val_loss: 0.0731 - val_accuracy: 0.9718\n",
      "Epoch 110/250\n",
      "1001/1001 - 8s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0731 - val_accuracy: 0.9718\n",
      "Epoch 111/250\n",
      "1001/1001 - 8s - loss: 0.0734 - accuracy: 0.9716 - val_loss: 0.0730 - val_accuracy: 0.9718\n",
      "Epoch 112/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9715 - val_loss: 0.0733 - val_accuracy: 0.9716\n",
      "Epoch 113/250\n",
      "1001/1001 - 8s - loss: 0.0734 - accuracy: 0.9716 - val_loss: 0.0729 - val_accuracy: 0.9719\n",
      "Epoch 114/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9717 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 115/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0729 - val_accuracy: 0.9718\n",
      "Epoch 116/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 117/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0737 - val_accuracy: 0.9717\n",
      "Epoch 118/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0728 - val_accuracy: 0.9719\n",
      "Epoch 119/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0733 - val_accuracy: 0.9717\n",
      "Epoch 120/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9717 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 121/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9718 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 122/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9718 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 123/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9718 - val_loss: 0.0727 - val_accuracy: 0.9719\n",
      "Epoch 124/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9718 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 125/250\n",
      "1001/1001 - 8s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 126/250\n",
      "1001/1001 - 8s - loss: 0.0730 - accuracy: 0.9718 - val_loss: 0.0725 - val_accuracy: 0.9721\n",
      "Epoch 127/250\n",
      "1001/1001 - 8s - loss: 0.0728 - accuracy: 0.9719 - val_loss: 0.0724 - val_accuracy: 0.9721\n",
      "Epoch 128/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9719 - val_loss: 0.0724 - val_accuracy: 0.9722\n",
      "Epoch 129/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0740 - val_accuracy: 0.9715\n",
      "Epoch 130/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9718 - val_loss: 0.0725 - val_accuracy: 0.9721\n",
      "Epoch 131/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0725 - val_accuracy: 0.9720\n",
      "Epoch 132/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0724 - val_accuracy: 0.9721\n",
      "Epoch 133/250\n",
      "1001/1001 - 8s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0724 - val_accuracy: 0.9721\n",
      "Epoch 134/250\n",
      "1001/1001 - 8s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0724 - val_accuracy: 0.9722\n",
      "Epoch 135/250\n",
      "1001/1001 - 8s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0724 - val_accuracy: 0.9721\n",
      "Epoch 136/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0723 - val_accuracy: 0.9722\n",
      "Epoch 137/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0727 - val_accuracy: 0.9720\n",
      "Epoch 138/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0724 - val_accuracy: 0.9721\n",
      "Epoch 139/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0722 - val_accuracy: 0.9721\n",
      "Epoch 140/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0723 - val_accuracy: 0.9721\n",
      "Epoch 141/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 142/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 143/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0723 - val_accuracy: 0.9721\n",
      "Epoch 144/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 145/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0723 - val_accuracy: 0.9722\n",
      "Epoch 146/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0723 - val_accuracy: 0.9722\n",
      "Epoch 147/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0723 - val_accuracy: 0.9722\n",
      "Epoch 148/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0723 - val_accuracy: 0.9722\n",
      "Epoch 149/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 150/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9721 - val_loss: 0.0723 - val_accuracy: 0.9721\n",
      "Epoch 151/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0722 - val_accuracy: 0.9721\n",
      "Epoch 152/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 153/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 154/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 155/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 156/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 157/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9721 - val_loss: 0.0725 - val_accuracy: 0.9720\n",
      "Epoch 158/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 159/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 160/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9721 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 161/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0724 - val_accuracy: 0.9721\n",
      "Epoch 162/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 163/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 164/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 165/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0723 - val_accuracy: 0.9722\n",
      "Epoch 166/250\n",
      "1001/1001 - 8s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0722 - val_accuracy: 0.9721\n",
      "Epoch 167/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 168/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 169/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 170/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 171/250\n",
      "1001/1001 - 8s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0723 - val_accuracy: 0.9721\n",
      "Epoch 172/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 173/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 174/250\n",
      "1001/1001 - 8s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 175/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9722 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 176/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9723\n",
      "Epoch 177/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 178/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 179/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 180/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9723\n",
      "Epoch 181/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 182/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
      "Epoch 183/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 184/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 185/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 186/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 187/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 188/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 189/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 190/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 191/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0721 - val_accuracy: 0.9723\n",
      "Epoch 192/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0723 - val_accuracy: 0.9722\n",
      "Epoch 193/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 194/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 195/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 196/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 197/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 198/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0719 - val_accuracy: 0.9722\n",
      "Epoch 199/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "\n",
      "Epoch 00200: starting stochastic weight averaging\n",
      "Epoch 200/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 201/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 202/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 203/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 204/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0729 - val_accuracy: 0.9719\n",
      "Epoch 205/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 206/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0719 - val_accuracy: 0.9723\n",
      "Epoch 207/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0719 - val_accuracy: 0.9723\n",
      "Epoch 208/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 209/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 210/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0719 - val_accuracy: 0.9723\n",
      "Epoch 211/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 212/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 213/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9723\n",
      "Epoch 214/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 215/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 216/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 217/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 218/250\n",
      "1001/1001 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 219/250\n",
      "1001/1001 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 220/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 221/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 222/250\n",
      "1001/1001 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 223/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 224/250\n",
      "1001/1001 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 225/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 226/250\n",
      "1001/1001 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 227/250\n",
      "1001/1001 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 228/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 229/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 230/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 231/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 232/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0722 - val_accuracy: 0.9721\n",
      "Epoch 233/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9723\n",
      "Epoch 234/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 235/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 236/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9724 - val_loss: 0.0720 - val_accuracy: 0.9722\n",
      "Epoch 237/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 238/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 239/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 240/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 241/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 242/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 243/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 244/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 245/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "Epoch 246/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 247/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0720 - val_accuracy: 0.9723\n",
      "Epoch 248/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0721 - val_accuracy: 0.9723\n",
      "Epoch 249/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0722 - val_accuracy: 0.9722\n",
      "Epoch 250/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00251: final model weights set to stochastic weight average\n",
      "Training fold 2 completed. macro f1 score : 0.94304\n",
      "Our training dataset shape is (1001, 4000, 51)\n",
      "Our validation dataset shape is (250, 4000, 51)\n",
      "Train on 1001 samples, validate on 250 samples\n",
      "Epoch 1/250\n",
      "1001/1001 - 17s - loss: 0.4097 - accuracy: 0.8666 - val_loss: 0.1262 - val_accuracy: 0.9556\n",
      "Epoch 2/250\n",
      "1001/1001 - 7s - loss: 0.1113 - accuracy: 0.9600 - val_loss: 0.0915 - val_accuracy: 0.9675\n",
      "Epoch 3/250\n",
      "1001/1001 - 7s - loss: 0.0894 - accuracy: 0.9675 - val_loss: 0.0854 - val_accuracy: 0.9686\n",
      "Epoch 4/250\n",
      "1001/1001 - 7s - loss: 0.0925 - accuracy: 0.9653 - val_loss: 0.1114 - val_accuracy: 0.9562\n",
      "Epoch 5/250\n",
      "1001/1001 - 7s - loss: 0.0972 - accuracy: 0.9641 - val_loss: 0.0953 - val_accuracy: 0.9649\n",
      "Epoch 6/250\n",
      "1001/1001 - 7s - loss: 0.0859 - accuracy: 0.9680 - val_loss: 0.0812 - val_accuracy: 0.9694\n",
      "Epoch 7/250\n",
      "1001/1001 - 7s - loss: 0.0821 - accuracy: 0.9691 - val_loss: 0.0962 - val_accuracy: 0.9642\n",
      "Epoch 8/250\n",
      "1001/1001 - 8s - loss: 0.0840 - accuracy: 0.9684 - val_loss: 0.0819 - val_accuracy: 0.9689\n",
      "Epoch 9/250\n",
      "1001/1001 - 8s - loss: 0.0811 - accuracy: 0.9692 - val_loss: 0.0795 - val_accuracy: 0.9698\n",
      "Epoch 10/250\n",
      "1001/1001 - 7s - loss: 0.0801 - accuracy: 0.9694 - val_loss: 0.0785 - val_accuracy: 0.9701\n",
      "Epoch 11/250\n",
      "1001/1001 - 7s - loss: 0.0784 - accuracy: 0.9700 - val_loss: 0.0780 - val_accuracy: 0.9702\n",
      "Epoch 12/250\n",
      "1001/1001 - 7s - loss: 0.0790 - accuracy: 0.9698 - val_loss: 0.0793 - val_accuracy: 0.9697\n",
      "Epoch 13/250\n",
      "1001/1001 - 8s - loss: 0.0780 - accuracy: 0.9701 - val_loss: 0.0792 - val_accuracy: 0.9696\n",
      "Epoch 14/250\n",
      "1001/1001 - 7s - loss: 0.0849 - accuracy: 0.9678 - val_loss: 0.0790 - val_accuracy: 0.9698\n",
      "Epoch 15/250\n",
      "1001/1001 - 7s - loss: 0.0796 - accuracy: 0.9694 - val_loss: 0.0891 - val_accuracy: 0.9662\n",
      "Epoch 16/250\n",
      "1001/1001 - 8s - loss: 0.1112 - accuracy: 0.9589 - val_loss: 0.0815 - val_accuracy: 0.9693\n",
      "Epoch 17/250\n",
      "1001/1001 - 8s - loss: 0.0811 - accuracy: 0.9693 - val_loss: 0.0780 - val_accuracy: 0.9701\n",
      "Epoch 18/250\n",
      "1001/1001 - 7s - loss: 0.0775 - accuracy: 0.9703 - val_loss: 0.0780 - val_accuracy: 0.9701\n",
      "Epoch 19/250\n",
      "1001/1001 - 7s - loss: 0.0785 - accuracy: 0.9699 - val_loss: 0.0775 - val_accuracy: 0.9702\n",
      "Epoch 20/250\n",
      "1001/1001 - 7s - loss: 0.0770 - accuracy: 0.9704 - val_loss: 0.0770 - val_accuracy: 0.9703\n",
      "Epoch 21/250\n",
      "1001/1001 - 7s - loss: 0.0800 - accuracy: 0.9694 - val_loss: 0.0771 - val_accuracy: 0.9704\n",
      "Epoch 22/250\n",
      "1001/1001 - 8s - loss: 0.0770 - accuracy: 0.9704 - val_loss: 0.0766 - val_accuracy: 0.9705\n",
      "Epoch 23/250\n",
      "1001/1001 - 7s - loss: 0.0779 - accuracy: 0.9700 - val_loss: 0.0978 - val_accuracy: 0.9629\n",
      "Epoch 24/250\n",
      "1001/1001 - 8s - loss: 0.0831 - accuracy: 0.9683 - val_loss: 0.0779 - val_accuracy: 0.9700\n",
      "Epoch 25/250\n",
      "1001/1001 - 8s - loss: 0.0768 - accuracy: 0.9704 - val_loss: 0.0768 - val_accuracy: 0.9704\n",
      "Epoch 26/250\n",
      "1001/1001 - 7s - loss: 0.0764 - accuracy: 0.9705 - val_loss: 0.0763 - val_accuracy: 0.9706\n",
      "Epoch 27/250\n",
      "1001/1001 - 7s - loss: 0.0769 - accuracy: 0.9703 - val_loss: 0.0784 - val_accuracy: 0.9699\n",
      "Epoch 28/250\n",
      "1001/1001 - 7s - loss: 0.0769 - accuracy: 0.9703 - val_loss: 0.0829 - val_accuracy: 0.9683\n",
      "Epoch 29/250\n",
      "1001/1001 - 7s - loss: 0.0771 - accuracy: 0.9702 - val_loss: 0.0764 - val_accuracy: 0.9705\n",
      "Epoch 30/250\n",
      "1001/1001 - 7s - loss: 0.0759 - accuracy: 0.9707 - val_loss: 0.0766 - val_accuracy: 0.9705\n",
      "Epoch 31/250\n",
      "1001/1001 - 7s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 32/250\n",
      "1001/1001 - 8s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 33/250\n",
      "1001/1001 - 8s - loss: 0.0753 - accuracy: 0.9708 - val_loss: 0.0755 - val_accuracy: 0.9707\n",
      "Epoch 34/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 35/250\n",
      "1001/1001 - 8s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0758 - val_accuracy: 0.9707\n",
      "Epoch 36/250\n",
      "1001/1001 - 7s - loss: 0.0755 - accuracy: 0.9708 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 37/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 38/250\n",
      "1001/1001 - 8s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 39/250\n",
      "1001/1001 - 7s - loss: 0.0760 - accuracy: 0.9706 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 40/250\n",
      "1001/1001 - 8s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0762 - val_accuracy: 0.9706\n",
      "Epoch 41/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 42/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0753 - val_accuracy: 0.9709\n",
      "Epoch 43/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0754 - val_accuracy: 0.9708\n",
      "Epoch 44/250\n",
      "1001/1001 - 8s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0753 - val_accuracy: 0.9709\n",
      "Epoch 45/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9710 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 46/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0756 - val_accuracy: 0.9707\n",
      "Epoch 47/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0755 - val_accuracy: 0.9707\n",
      "Epoch 48/250\n",
      "1001/1001 - 8s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0754 - val_accuracy: 0.9708\n",
      "Epoch 49/250\n",
      "1001/1001 - 8s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0756 - val_accuracy: 0.9707\n",
      "Epoch 50/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.0755 - val_accuracy: 0.9707\n",
      "Epoch 51/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 52/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 53/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0752 - val_accuracy: 0.9708\n",
      "Epoch 54/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0756 - val_accuracy: 0.9707\n",
      "Epoch 55/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0752 - val_accuracy: 0.9709\n",
      "Epoch 56/250\n",
      "1001/1001 - 8s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0753 - val_accuracy: 0.9709\n",
      "Epoch 57/250\n",
      "1001/1001 - 8s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0760 - val_accuracy: 0.9706\n",
      "Epoch 58/250\n",
      "1001/1001 - 8s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 59/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0752 - val_accuracy: 0.9709\n",
      "Epoch 60/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0752 - val_accuracy: 0.9708\n",
      "Epoch 61/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0754 - val_accuracy: 0.9708\n",
      "Epoch 62/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0752 - val_accuracy: 0.9709\n",
      "Epoch 63/250\n",
      "1001/1001 - 8s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0752 - val_accuracy: 0.9709\n",
      "Epoch 64/250\n",
      "1001/1001 - 8s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 65/250\n",
      "1001/1001 - 8s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 66/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 67/250\n",
      "1001/1001 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 68/250\n",
      "1001/1001 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 69/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0754 - val_accuracy: 0.9708\n",
      "Epoch 70/250\n",
      "1001/1001 - 8s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0752 - val_accuracy: 0.9708\n",
      "Epoch 71/250\n",
      "1001/1001 - 7s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 72/250\n",
      "1001/1001 - 8s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 73/250\n",
      "1001/1001 - 8s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 74/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0751 - val_accuracy: 0.9708\n",
      "Epoch 75/250\n",
      "1001/1001 - 7s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 76/250\n",
      "1001/1001 - 8s - loss: 0.0742 - accuracy: 0.9713 - val_loss: 0.0752 - val_accuracy: 0.9708\n",
      "Epoch 77/250\n",
      "1001/1001 - 8s - loss: 0.0741 - accuracy: 0.9712 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 78/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9712 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 79/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0752 - val_accuracy: 0.9708\n",
      "Epoch 80/250\n",
      "1001/1001 - 8s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0752 - val_accuracy: 0.9709\n",
      "Epoch 81/250\n",
      "1001/1001 - 8s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 82/250\n",
      "1001/1001 - 7s - loss: 0.0739 - accuracy: 0.9713 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 83/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0756 - val_accuracy: 0.9707\n",
      "Epoch 84/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0752 - val_accuracy: 0.9708\n",
      "Epoch 85/250\n",
      "1001/1001 - 7s - loss: 0.0739 - accuracy: 0.9714 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 86/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0753 - val_accuracy: 0.9708\n",
      "Epoch 87/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0751 - val_accuracy: 0.9710\n",
      "Epoch 88/250\n",
      "1001/1001 - 8s - loss: 0.0739 - accuracy: 0.9714 - val_loss: 0.0751 - val_accuracy: 0.9709\n",
      "Epoch 89/250\n",
      "1001/1001 - 8s - loss: 0.0739 - accuracy: 0.9713 - val_loss: 0.0754 - val_accuracy: 0.9707\n",
      "Epoch 90/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0751 - val_accuracy: 0.9710\n",
      "Epoch 91/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9714 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 92/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 93/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 94/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 95/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9715 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 96/250\n",
      "1001/1001 - 8s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 97/250\n",
      "1001/1001 - 8s - loss: 0.0735 - accuracy: 0.9715 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 98/250\n",
      "1001/1001 - 7s - loss: 0.0734 - accuracy: 0.9716 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 99/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 100/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0747 - val_accuracy: 0.9710\n",
      "Epoch 101/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 102/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 103/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9716 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 104/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9716 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 105/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 106/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 107/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 108/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 109/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 110/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 111/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 112/250\n",
      "1001/1001 - 8s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0748 - val_accuracy: 0.9710\n",
      "Epoch 113/250\n",
      "1001/1001 - 8s - loss: 0.0729 - accuracy: 0.9717 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 114/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 115/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 116/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 117/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 118/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 119/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 120/250\n",
      "1001/1001 - 8s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 121/250\n",
      "1001/1001 - 8s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0744 - val_accuracy: 0.9714\n",
      "Epoch 122/250\n",
      "1001/1001 - 8s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 123/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 124/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9718 - val_loss: 0.0744 - val_accuracy: 0.9711\n",
      "Epoch 125/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 126/250\n",
      "1001/1001 - 8s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 127/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 128/250\n",
      "1001/1001 - 8s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 129/250\n",
      "1001/1001 - 8s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 130/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9718 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 131/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 132/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 133/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 134/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 135/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 136/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 137/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 138/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 139/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 140/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 141/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 142/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 143/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 144/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9719 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 145/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 146/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 147/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 148/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 149/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 150/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 151/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 152/250\n",
      "1001/1001 - 8s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 153/250\n",
      "1001/1001 - 8s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 154/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 155/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 156/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 157/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 158/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9720 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 159/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 160/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 161/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 162/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 163/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 164/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 165/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 166/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 167/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 168/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 169/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 170/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 171/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 172/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 173/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 174/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 175/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.0744 - val_accuracy: 0.9713\n",
      "Epoch 176/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0742 - val_accuracy: 0.9713\n",
      "Epoch 177/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 178/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 179/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 180/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 181/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 182/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 183/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 184/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0745 - val_accuracy: 0.9711\n",
      "Epoch 185/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 186/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 187/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 188/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 189/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 190/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 191/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 192/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 193/250\n",
      "1001/1001 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 194/250\n",
      "1001/1001 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 195/250\n",
      "1001/1001 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0743 - val_accuracy: 0.9713\n",
      "Epoch 196/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0743 - val_accuracy: 0.9712\n",
      "Epoch 197/250\n",
      "1001/1001 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 198/250\n",
      "1001/1001 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 199/250\n",
      "1001/1001 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00200: starting stochastic weight averaging\n",
      "Epoch 200/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0744 - val_accuracy: 0.9713\n",
      "Epoch 201/250\n",
      "1001/1001 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 202/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 203/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 204/250\n",
      "1001/1001 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 205/250\n",
      "1001/1001 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 206/250\n",
      "1001/1001 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 207/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 208/250\n",
      "1001/1001 - 8s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 209/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0744 - val_accuracy: 0.9713\n",
      "Epoch 210/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 211/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 212/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 213/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 214/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 215/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 216/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 217/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 218/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
      "Epoch 219/250\n",
      "1001/1001 - 8s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 220/250\n",
      "1001/1001 - 7s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 221/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 222/250\n",
      "1001/1001 - 7s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 223/250\n",
      "1001/1001 - 8s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0745 - val_accuracy: 0.9711\n",
      "Epoch 224/250\n",
      "1001/1001 - 8s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 225/250\n",
      "1001/1001 - 8s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 226/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 227/250\n",
      "1001/1001 - 7s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 228/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 229/250\n",
      "1001/1001 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0745 - val_accuracy: 0.9712\n",
      "Epoch 230/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 231/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 232/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 233/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 234/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.0749 - val_accuracy: 0.9711\n",
      "Epoch 235/250\n",
      "1001/1001 - 7s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 236/250\n",
      "1001/1001 - 8s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 237/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.0753 - val_accuracy: 0.9709\n",
      "Epoch 238/250\n",
      "1001/1001 - 7s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 239/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 240/250\n",
      "1001/1001 - 8s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 241/250\n",
      "1001/1001 - 8s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 242/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0746 - val_accuracy: 0.9711\n",
      "Epoch 243/250\n",
      "1001/1001 - 7s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 244/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0746 - val_accuracy: 0.9712\n",
      "Epoch 245/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0749 - val_accuracy: 0.9710\n",
      "Epoch 246/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 247/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 248/250\n",
      "1001/1001 - 8s - loss: 0.0708 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 249/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "Epoch 250/250\n",
      "1001/1001 - 7s - loss: 0.0708 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9711\n",
      "\n",
      "Epoch 00251: final model weights set to stochastic weight average\n",
      "Training fold 3 completed. macro f1 score : 0.94096\n",
      "Our training dataset shape is (1001, 4000, 51)\n",
      "Our validation dataset shape is (250, 4000, 51)\n",
      "Train on 1001 samples, validate on 250 samples\n",
      "Epoch 1/250\n",
      "1001/1001 - 16s - loss: 0.6400 - accuracy: 0.7822 - val_loss: 0.2068 - val_accuracy: 0.9304\n",
      "Epoch 2/250\n",
      "1001/1001 - 7s - loss: 0.1389 - accuracy: 0.9543 - val_loss: 0.1032 - val_accuracy: 0.9656\n",
      "Epoch 3/250\n",
      "1001/1001 - 8s - loss: 0.0969 - accuracy: 0.9658 - val_loss: 0.1038 - val_accuracy: 0.9618\n",
      "Epoch 4/250\n",
      "1001/1001 - 8s - loss: 0.0906 - accuracy: 0.9670 - val_loss: 0.0853 - val_accuracy: 0.9687\n",
      "Epoch 5/250\n",
      "1001/1001 - 8s - loss: 0.0843 - accuracy: 0.9687 - val_loss: 0.0850 - val_accuracy: 0.9684\n",
      "Epoch 6/250\n",
      "1001/1001 - 7s - loss: 0.0846 - accuracy: 0.9684 - val_loss: 0.1043 - val_accuracy: 0.9615\n",
      "Epoch 7/250\n",
      "1001/1001 - 7s - loss: 0.1024 - accuracy: 0.9619 - val_loss: 0.1034 - val_accuracy: 0.9613\n",
      "Epoch 8/250\n",
      "1001/1001 - 7s - loss: 0.0872 - accuracy: 0.9676 - val_loss: 0.0830 - val_accuracy: 0.9691\n",
      "Epoch 9/250\n",
      "1001/1001 - 7s - loss: 0.0809 - accuracy: 0.9695 - val_loss: 0.0810 - val_accuracy: 0.9694\n",
      "Epoch 10/250\n",
      "1001/1001 - 7s - loss: 0.0792 - accuracy: 0.9699 - val_loss: 0.0811 - val_accuracy: 0.9692\n",
      "Epoch 11/250\n",
      "1001/1001 - 8s - loss: 0.0795 - accuracy: 0.9697 - val_loss: 0.0807 - val_accuracy: 0.9694\n",
      "Epoch 12/250\n",
      "1001/1001 - 8s - loss: 0.0848 - accuracy: 0.9679 - val_loss: 0.0903 - val_accuracy: 0.9662\n",
      "Epoch 13/250\n",
      "1001/1001 - 7s - loss: 0.0818 - accuracy: 0.9690 - val_loss: 0.0810 - val_accuracy: 0.9691\n",
      "Epoch 14/250\n",
      "1001/1001 - 7s - loss: 0.0862 - accuracy: 0.9676 - val_loss: 0.0811 - val_accuracy: 0.9692\n",
      "Epoch 15/250\n",
      "1001/1001 - 8s - loss: 0.0815 - accuracy: 0.9691 - val_loss: 0.0801 - val_accuracy: 0.9695\n",
      "Epoch 16/250\n",
      "1001/1001 - 7s - loss: 0.0793 - accuracy: 0.9697 - val_loss: 0.0796 - val_accuracy: 0.9695\n",
      "Epoch 17/250\n",
      "1001/1001 - 7s - loss: 0.0784 - accuracy: 0.9700 - val_loss: 0.0794 - val_accuracy: 0.9697\n",
      "Epoch 18/250\n",
      "1001/1001 - 7s - loss: 0.0777 - accuracy: 0.9701 - val_loss: 0.0916 - val_accuracy: 0.9657\n",
      "Epoch 19/250\n",
      "1001/1001 - 8s - loss: 0.0815 - accuracy: 0.9689 - val_loss: 0.0794 - val_accuracy: 0.9696\n",
      "Epoch 20/250\n",
      "1001/1001 - 8s - loss: 0.0774 - accuracy: 0.9702 - val_loss: 0.0784 - val_accuracy: 0.9699\n",
      "Epoch 21/250\n",
      "1001/1001 - 7s - loss: 0.1311 - accuracy: 0.9524 - val_loss: 0.0946 - val_accuracy: 0.9658\n",
      "Epoch 22/250\n",
      "1001/1001 - 7s - loss: 0.0823 - accuracy: 0.9693 - val_loss: 0.0807 - val_accuracy: 0.9695\n",
      "Epoch 23/250\n",
      "1001/1001 - 8s - loss: 0.0784 - accuracy: 0.9702 - val_loss: 0.0807 - val_accuracy: 0.9694\n",
      "Epoch 24/250\n",
      "1001/1001 - 7s - loss: 0.0782 - accuracy: 0.9701 - val_loss: 0.0788 - val_accuracy: 0.9699\n",
      "Epoch 25/250\n",
      "1001/1001 - 7s - loss: 0.0775 - accuracy: 0.9703 - val_loss: 0.0786 - val_accuracy: 0.9700\n",
      "Epoch 26/250\n",
      "1001/1001 - 7s - loss: 0.0770 - accuracy: 0.9704 - val_loss: 0.0786 - val_accuracy: 0.9699\n",
      "Epoch 27/250\n",
      "1001/1001 - 8s - loss: 0.0769 - accuracy: 0.9704 - val_loss: 0.0785 - val_accuracy: 0.9698\n",
      "Epoch 28/250\n",
      "1001/1001 - 8s - loss: 0.0769 - accuracy: 0.9704 - val_loss: 0.0795 - val_accuracy: 0.9696\n",
      "Epoch 29/250\n",
      "1001/1001 - 8s - loss: 0.0781 - accuracy: 0.9701 - val_loss: 0.0794 - val_accuracy: 0.9696\n",
      "Epoch 30/250\n",
      "1001/1001 - 7s - loss: 0.0765 - accuracy: 0.9705 - val_loss: 0.0779 - val_accuracy: 0.9700\n",
      "Epoch 31/250\n",
      "1001/1001 - 8s - loss: 0.0756 - accuracy: 0.9708 - val_loss: 0.0773 - val_accuracy: 0.9702\n",
      "Epoch 32/250\n",
      "1001/1001 - 8s - loss: 0.0756 - accuracy: 0.9708 - val_loss: 0.0773 - val_accuracy: 0.9703\n",
      "Epoch 33/250\n",
      "1001/1001 - 8s - loss: 0.0756 - accuracy: 0.9708 - val_loss: 0.0774 - val_accuracy: 0.9702\n",
      "Epoch 34/250\n",
      "1001/1001 - 7s - loss: 0.0755 - accuracy: 0.9708 - val_loss: 0.0773 - val_accuracy: 0.9702\n",
      "Epoch 35/250\n",
      "1001/1001 - 8s - loss: 0.0755 - accuracy: 0.9709 - val_loss: 0.0772 - val_accuracy: 0.9702\n",
      "Epoch 36/250\n",
      "1001/1001 - 8s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0771 - val_accuracy: 0.9703\n",
      "Epoch 37/250\n",
      "1001/1001 - 8s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0771 - val_accuracy: 0.9703\n",
      "Epoch 38/250\n",
      "1001/1001 - 7s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0772 - val_accuracy: 0.9702\n",
      "Epoch 39/250\n",
      "1001/1001 - 7s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0772 - val_accuracy: 0.9703\n",
      "Epoch 40/250\n",
      "1001/1001 - 7s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0774 - val_accuracy: 0.9702\n",
      "Epoch 41/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0769 - val_accuracy: 0.9703\n",
      "Epoch 42/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0770 - val_accuracy: 0.9704\n",
      "Epoch 43/250\n",
      "1001/1001 - 8s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0771 - val_accuracy: 0.9703\n",
      "Epoch 44/250\n",
      "1001/1001 - 8s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0769 - val_accuracy: 0.9703\n",
      "Epoch 45/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0769 - val_accuracy: 0.9704\n",
      "Epoch 46/250\n",
      "1001/1001 - 8s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0770 - val_accuracy: 0.9704\n",
      "Epoch 47/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0769 - val_accuracy: 0.9704\n",
      "Epoch 48/250\n",
      "1001/1001 - 7s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0768 - val_accuracy: 0.9704\n",
      "Epoch 49/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0770 - val_accuracy: 0.9703\n",
      "Epoch 50/250\n",
      "1001/1001 - 8s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0772 - val_accuracy: 0.9702\n",
      "Epoch 51/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0767 - val_accuracy: 0.9704\n",
      "Epoch 52/250\n",
      "1001/1001 - 8s - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.0769 - val_accuracy: 0.9704\n",
      "Epoch 53/250\n",
      "1001/1001 - 8s - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.0768 - val_accuracy: 0.9704\n",
      "Epoch 54/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0774 - val_accuracy: 0.9702\n",
      "Epoch 55/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0767 - val_accuracy: 0.9705\n",
      "Epoch 56/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.0771 - val_accuracy: 0.9703\n",
      "Epoch 57/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0772 - val_accuracy: 0.9703\n",
      "Epoch 58/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0766 - val_accuracy: 0.9705\n",
      "Epoch 59/250\n",
      "1001/1001 - 8s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0770 - val_accuracy: 0.9703\n",
      "Epoch 60/250\n",
      "1001/1001 - 8s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0767 - val_accuracy: 0.9704\n",
      "Epoch 61/250\n",
      "1001/1001 - 8s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0767 - val_accuracy: 0.9704\n",
      "Epoch 62/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9712 - val_loss: 0.0766 - val_accuracy: 0.9704\n",
      "Epoch 63/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0766 - val_accuracy: 0.9704\n",
      "Epoch 64/250\n",
      "1001/1001 - 8s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0766 - val_accuracy: 0.9704\n",
      "Epoch 65/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0768 - val_accuracy: 0.9704\n",
      "Epoch 66/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0768 - val_accuracy: 0.9705\n",
      "Epoch 67/250\n",
      "1001/1001 - 8s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0766 - val_accuracy: 0.9704\n",
      "Epoch 68/250\n",
      "1001/1001 - 8s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0765 - val_accuracy: 0.9705\n",
      "Epoch 69/250\n",
      "1001/1001 - 8s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0766 - val_accuracy: 0.9704\n",
      "Epoch 70/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9712 - val_loss: 0.0766 - val_accuracy: 0.9704\n",
      "Epoch 71/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0765 - val_accuracy: 0.9705\n",
      "Epoch 72/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0765 - val_accuracy: 0.9705\n",
      "Epoch 73/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9713 - val_loss: 0.0764 - val_accuracy: 0.9705\n",
      "Epoch 74/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0766 - val_accuracy: 0.9704\n",
      "Epoch 75/250\n",
      "1001/1001 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0766 - val_accuracy: 0.9705\n",
      "Epoch 76/250\n",
      "1001/1001 - 8s - loss: 0.0743 - accuracy: 0.9713 - val_loss: 0.0765 - val_accuracy: 0.9705\n",
      "Epoch 77/250\n",
      "1001/1001 - 8s - loss: 0.0742 - accuracy: 0.9713 - val_loss: 0.0764 - val_accuracy: 0.9704\n",
      "Epoch 78/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0769 - val_accuracy: 0.9703\n",
      "Epoch 79/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0764 - val_accuracy: 0.9705\n",
      "Epoch 80/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9713 - val_loss: 0.0763 - val_accuracy: 0.9705\n",
      "Epoch 81/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0763 - val_accuracy: 0.9706\n",
      "Epoch 82/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9714 - val_loss: 0.0763 - val_accuracy: 0.9705\n",
      "Epoch 83/250\n",
      "1001/1001 - 8s - loss: 0.0740 - accuracy: 0.9714 - val_loss: 0.0763 - val_accuracy: 0.9705\n",
      "Epoch 84/250\n",
      "1001/1001 - 8s - loss: 0.0739 - accuracy: 0.9714 - val_loss: 0.0765 - val_accuracy: 0.9705\n",
      "Epoch 85/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0762 - val_accuracy: 0.9707\n",
      "Epoch 86/250\n",
      "1001/1001 - 8s - loss: 0.0739 - accuracy: 0.9715 - val_loss: 0.0761 - val_accuracy: 0.9706\n",
      "Epoch 87/250\n",
      "1001/1001 - 7s - loss: 0.0737 - accuracy: 0.9714 - val_loss: 0.0761 - val_accuracy: 0.9707\n",
      "Epoch 88/250\n",
      "1001/1001 - 7s - loss: 0.0737 - accuracy: 0.9715 - val_loss: 0.0761 - val_accuracy: 0.9707\n",
      "Epoch 89/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9715 - val_loss: 0.0760 - val_accuracy: 0.9707\n",
      "Epoch 90/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0764 - val_accuracy: 0.9705\n",
      "Epoch 91/250\n",
      "1001/1001 - 8s - loss: 0.0737 - accuracy: 0.9715 - val_loss: 0.0759 - val_accuracy: 0.9708\n",
      "Epoch 92/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0760 - val_accuracy: 0.9707\n",
      "Epoch 93/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 94/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9716 - val_loss: 0.0761 - val_accuracy: 0.9707\n",
      "Epoch 95/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 96/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9716 - val_loss: 0.0758 - val_accuracy: 0.9707\n",
      "Epoch 97/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9716 - val_loss: 0.0758 - val_accuracy: 0.9707\n",
      "Epoch 98/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9716 - val_loss: 0.0759 - val_accuracy: 0.9708\n",
      "Epoch 99/250\n",
      "1001/1001 - 8s - loss: 0.0734 - accuracy: 0.9716 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 100/250\n",
      "1001/1001 - 8s - loss: 0.0735 - accuracy: 0.9716 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 101/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9716 - val_loss: 0.0760 - val_accuracy: 0.9708\n",
      "Epoch 102/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 103/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9716 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 104/250\n",
      "1001/1001 - 7s - loss: 0.0734 - accuracy: 0.9716 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 105/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9717 - val_loss: 0.0759 - val_accuracy: 0.9708\n",
      "Epoch 106/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9717 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 107/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 108/250\n",
      "1001/1001 - 8s - loss: 0.0733 - accuracy: 0.9717 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 109/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0758 - val_accuracy: 0.9709\n",
      "Epoch 110/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0762 - val_accuracy: 0.9706\n",
      "Epoch 111/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 112/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9717 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 113/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0759 - val_accuracy: 0.9708\n",
      "Epoch 114/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 115/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0763 - val_accuracy: 0.9706\n",
      "Epoch 116/250\n",
      "1001/1001 - 8s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 117/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9718 - val_loss: 0.0757 - val_accuracy: 0.9708\n",
      "Epoch 118/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 119/250\n",
      "1001/1001 - 7s - loss: 0.0733 - accuracy: 0.9717 - val_loss: 0.0757 - val_accuracy: 0.9708\n",
      "Epoch 120/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 121/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9718 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 122/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 123/250\n",
      "1001/1001 - 8s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0759 - val_accuracy: 0.9708\n",
      "Epoch 124/250\n",
      "1001/1001 - 8s - loss: 0.0730 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 125/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 126/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 127/250\n",
      "1001/1001 - 8s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 128/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0757 - val_accuracy: 0.9708\n",
      "Epoch 129/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9718 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 130/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 131/250\n",
      "1001/1001 - 8s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 132/250\n",
      "1001/1001 - 8s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 133/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 134/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 135/250\n",
      "1001/1001 - 8s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 136/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 137/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 138/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 139/250\n",
      "1001/1001 - 8s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 140/250\n",
      "1001/1001 - 8s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 141/250\n",
      "1001/1001 - 8s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 142/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 143/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 144/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 145/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9718 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 146/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 147/250\n",
      "1001/1001 - 8s - loss: 0.0727 - accuracy: 0.9718 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 148/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 149/250\n",
      "1001/1001 - 8s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 150/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 151/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 152/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 153/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 154/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 155/250\n",
      "1001/1001 - 8s - loss: 0.0726 - accuracy: 0.9718 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 156/250\n",
      "1001/1001 - 8s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 157/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9719 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 158/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 159/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 160/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 161/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 162/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 163/250\n",
      "1001/1001 - 8s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 164/250\n",
      "1001/1001 - 8s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 165/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0757 - val_accuracy: 0.9708\n",
      "Epoch 166/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 167/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 168/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 169/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 170/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 171/250\n",
      "1001/1001 - 8s - loss: 0.0724 - accuracy: 0.9719 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 172/250\n",
      "1001/1001 - 8s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 173/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 174/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 175/250\n",
      "1001/1001 - 7s - loss: 0.0726 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 176/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 177/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 178/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 179/250\n",
      "1001/1001 - 8s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0760 - val_accuracy: 0.9707\n",
      "Epoch 180/250\n",
      "1001/1001 - 8s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0757 - val_accuracy: 0.9708\n",
      "Epoch 181/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0757 - val_accuracy: 0.9708\n",
      "Epoch 182/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0758 - val_accuracy: 0.9708\n",
      "Epoch 183/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9719 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 184/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 185/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 186/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 187/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0756 - val_accuracy: 0.9710\n",
      "Epoch 188/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0762 - val_accuracy: 0.9706\n",
      "Epoch 189/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 190/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 191/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 192/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 193/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 194/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 195/250\n",
      "1001/1001 - 8s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 196/250\n",
      "1001/1001 - 8s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 197/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 198/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 199/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00200: starting stochastic weight averaging\n",
      "Epoch 200/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 201/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 202/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 203/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 204/250\n",
      "1001/1001 - 8s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 205/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 206/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0759 - val_accuracy: 0.9708\n",
      "Epoch 207/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 208/250\n",
      "1001/1001 - 7s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 209/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 210/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 211/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 212/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 213/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 214/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 215/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 216/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 217/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 218/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 219/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 220/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 221/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 222/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 223/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0756 - val_accuracy: 0.9710\n",
      "Epoch 224/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 225/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 226/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 227/250\n",
      "1001/1001 - 8s - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.0763 - val_accuracy: 0.9706\n",
      "Epoch 228/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 229/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 230/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 231/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 232/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 233/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 234/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0759 - val_accuracy: 0.9708\n",
      "Epoch 235/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 236/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 237/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 238/250\n",
      "1001/1001 - 8s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 239/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 240/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 241/250\n",
      "1001/1001 - 8s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9709\n",
      "Epoch 242/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 243/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 244/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 245/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0754 - val_accuracy: 0.9710\n",
      "Epoch 246/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0756 - val_accuracy: 0.9709\n",
      "Epoch 247/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 248/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 249/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 250/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9722 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "\n",
      "Epoch 00251: final model weights set to stochastic weight average\n",
      "Training fold 4 completed. macro f1 score : 0.94156\n",
      "Our training dataset shape is (1001, 4000, 51)\n",
      "Our validation dataset shape is (250, 4000, 51)\n",
      "Train on 1001 samples, validate on 250 samples\n",
      "Epoch 1/250\n",
      "1001/1001 - 17s - loss: 0.4880 - accuracy: 0.8446 - val_loss: 0.1616 - val_accuracy: 0.9482\n",
      "Epoch 2/250\n",
      "1001/1001 - 7s - loss: 0.1168 - accuracy: 0.9595 - val_loss: 0.0972 - val_accuracy: 0.9645\n",
      "Epoch 3/250\n",
      "1001/1001 - 7s - loss: 0.0976 - accuracy: 0.9636 - val_loss: 0.0945 - val_accuracy: 0.9646\n",
      "Epoch 4/250\n",
      "1001/1001 - 8s - loss: 0.0951 - accuracy: 0.9643 - val_loss: 0.0914 - val_accuracy: 0.9657\n",
      "Epoch 5/250\n",
      "1001/1001 - 7s - loss: 0.0879 - accuracy: 0.9669 - val_loss: 0.1026 - val_accuracy: 0.9613\n",
      "Epoch 6/250\n",
      "1001/1001 - 8s - loss: 0.0888 - accuracy: 0.9666 - val_loss: 0.0851 - val_accuracy: 0.9677\n",
      "Epoch 7/250\n",
      "1001/1001 - 8s - loss: 0.0892 - accuracy: 0.9663 - val_loss: 0.1424 - val_accuracy: 0.9431\n",
      "Epoch 8/250\n",
      "1001/1001 - 7s - loss: 0.0962 - accuracy: 0.9642 - val_loss: 0.0837 - val_accuracy: 0.9684\n",
      "Epoch 9/250\n",
      "1001/1001 - 8s - loss: 0.0812 - accuracy: 0.9693 - val_loss: 0.0809 - val_accuracy: 0.9692\n",
      "Epoch 10/250\n",
      "1001/1001 - 7s - loss: 0.0792 - accuracy: 0.9698 - val_loss: 0.0828 - val_accuracy: 0.9687\n",
      "Epoch 11/250\n",
      "1001/1001 - 7s - loss: 0.0788 - accuracy: 0.9699 - val_loss: 0.0810 - val_accuracy: 0.9690\n",
      "Epoch 12/250\n",
      "1001/1001 - 7s - loss: 0.0785 - accuracy: 0.9699 - val_loss: 0.0809 - val_accuracy: 0.9691\n",
      "Epoch 13/250\n",
      "1001/1001 - 8s - loss: 0.0818 - accuracy: 0.9688 - val_loss: 0.0822 - val_accuracy: 0.9687\n",
      "Epoch 14/250\n",
      "1001/1001 - 8s - loss: 0.0800 - accuracy: 0.9694 - val_loss: 0.0831 - val_accuracy: 0.9683\n",
      "Epoch 15/250\n",
      "1001/1001 - 8s - loss: 0.0797 - accuracy: 0.9694 - val_loss: 0.0813 - val_accuracy: 0.9687\n",
      "Epoch 16/250\n",
      "1001/1001 - 8s - loss: 0.0963 - accuracy: 0.9633 - val_loss: 0.0875 - val_accuracy: 0.9671\n",
      "Epoch 17/250\n",
      "1001/1001 - 7s - loss: 0.0802 - accuracy: 0.9694 - val_loss: 0.0812 - val_accuracy: 0.9689\n",
      "Epoch 18/250\n",
      "1001/1001 - 8s - loss: 0.0780 - accuracy: 0.9701 - val_loss: 0.0833 - val_accuracy: 0.9679\n",
      "Epoch 19/250\n",
      "1001/1001 - 7s - loss: 0.0808 - accuracy: 0.9691 - val_loss: 0.0806 - val_accuracy: 0.9692\n",
      "Epoch 20/250\n",
      "1001/1001 - 7s - loss: 0.0777 - accuracy: 0.9701 - val_loss: 0.0804 - val_accuracy: 0.9692\n",
      "Epoch 21/250\n",
      "1001/1001 - 7s - loss: 0.0794 - accuracy: 0.9695 - val_loss: 0.0789 - val_accuracy: 0.9696\n",
      "Epoch 22/250\n",
      "1001/1001 - 8s - loss: 0.0789 - accuracy: 0.9697 - val_loss: 0.0790 - val_accuracy: 0.9696\n",
      "Epoch 23/250\n",
      "1001/1001 - 8s - loss: 0.0794 - accuracy: 0.9695 - val_loss: 0.1092 - val_accuracy: 0.9584\n",
      "Epoch 24/250\n",
      "1001/1001 - 7s - loss: 0.0834 - accuracy: 0.9682 - val_loss: 0.0804 - val_accuracy: 0.9692\n",
      "Epoch 25/250\n",
      "1001/1001 - 7s - loss: 0.0780 - accuracy: 0.9700 - val_loss: 0.0799 - val_accuracy: 0.9691\n",
      "Epoch 26/250\n",
      "1001/1001 - 7s - loss: 0.0769 - accuracy: 0.9704 - val_loss: 0.0785 - val_accuracy: 0.9698\n",
      "Epoch 27/250\n",
      "1001/1001 - 7s - loss: 0.0761 - accuracy: 0.9706 - val_loss: 0.0795 - val_accuracy: 0.9694\n",
      "Epoch 28/250\n",
      "1001/1001 - 7s - loss: 0.0788 - accuracy: 0.9696 - val_loss: 0.0785 - val_accuracy: 0.9697\n",
      "Epoch 29/250\n",
      "1001/1001 - 7s - loss: 0.0766 - accuracy: 0.9705 - val_loss: 0.0789 - val_accuracy: 0.9695\n",
      "Epoch 30/250\n",
      "1001/1001 - 8s - loss: 0.0764 - accuracy: 0.9705 - val_loss: 0.0780 - val_accuracy: 0.9699\n",
      "Epoch 31/250\n",
      "1001/1001 - 8s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0775 - val_accuracy: 0.9701\n",
      "Epoch 32/250\n",
      "1001/1001 - 7s - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0774 - val_accuracy: 0.9701\n",
      "Epoch 33/250\n",
      "1001/1001 - 7s - loss: 0.0755 - accuracy: 0.9708 - val_loss: 0.0775 - val_accuracy: 0.9701\n",
      "Epoch 34/250\n",
      "1001/1001 - 8s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0774 - val_accuracy: 0.9701\n",
      "Epoch 35/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0773 - val_accuracy: 0.9701\n",
      "Epoch 36/250\n",
      "1001/1001 - 7s - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.0774 - val_accuracy: 0.9700\n",
      "Epoch 37/250\n",
      "1001/1001 - 7s - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0775 - val_accuracy: 0.9700\n",
      "Epoch 38/250\n",
      "1001/1001 - 8s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0774 - val_accuracy: 0.9700\n",
      "Epoch 39/250\n",
      "1001/1001 - 8s - loss: 0.0757 - accuracy: 0.9707 - val_loss: 0.0774 - val_accuracy: 0.9700\n",
      "Epoch 40/250\n",
      "1001/1001 - 7s - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0773 - val_accuracy: 0.9701\n",
      "Epoch 41/250\n",
      "1001/1001 - 7s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0771 - val_accuracy: 0.9702\n",
      "Epoch 42/250\n",
      "1001/1001 - 8s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0772 - val_accuracy: 0.9701\n",
      "Epoch 43/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0773 - val_accuracy: 0.9702\n",
      "Epoch 44/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.0771 - val_accuracy: 0.9702\n",
      "Epoch 45/250\n",
      "1001/1001 - 7s - loss: 0.0748 - accuracy: 0.9710 - val_loss: 0.0772 - val_accuracy: 0.9701\n",
      "Epoch 46/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0774 - val_accuracy: 0.9701\n",
      "Epoch 47/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0771 - val_accuracy: 0.9702\n",
      "Epoch 48/250\n",
      "1001/1001 - 7s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0771 - val_accuracy: 0.9701\n",
      "Epoch 49/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0772 - val_accuracy: 0.9701\n",
      "Epoch 50/250\n",
      "1001/1001 - 7s - loss: 0.0750 - accuracy: 0.9710 - val_loss: 0.0772 - val_accuracy: 0.9701\n",
      "Epoch 51/250\n",
      "1001/1001 - 7s - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0770 - val_accuracy: 0.9702\n",
      "Epoch 52/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0770 - val_accuracy: 0.9702\n",
      "Epoch 53/250\n",
      "1001/1001 - 7s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0771 - val_accuracy: 0.9701\n",
      "Epoch 54/250\n",
      "1001/1001 - 8s - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0781 - val_accuracy: 0.9698\n",
      "Epoch 55/250\n",
      "1001/1001 - 8s - loss: 0.0749 - accuracy: 0.9710 - val_loss: 0.0769 - val_accuracy: 0.9702\n",
      "Epoch 56/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0773 - val_accuracy: 0.9701\n",
      "Epoch 57/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0773 - val_accuracy: 0.9700\n",
      "Epoch 58/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0770 - val_accuracy: 0.9702\n",
      "Epoch 59/250\n",
      "1001/1001 - 7s - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.0770 - val_accuracy: 0.9702\n",
      "Epoch 60/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0770 - val_accuracy: 0.9702\n",
      "Epoch 61/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0769 - val_accuracy: 0.9702\n",
      "Epoch 62/250\n",
      "1001/1001 - 8s - loss: 0.0744 - accuracy: 0.9711 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 63/250\n",
      "1001/1001 - 8s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0769 - val_accuracy: 0.9702\n",
      "Epoch 64/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 65/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0769 - val_accuracy: 0.9702\n",
      "Epoch 66/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0769 - val_accuracy: 0.9702\n",
      "Epoch 67/250\n",
      "1001/1001 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0769 - val_accuracy: 0.9702\n",
      "Epoch 68/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0767 - val_accuracy: 0.9703\n",
      "Epoch 69/250\n",
      "1001/1001 - 7s - loss: 0.0744 - accuracy: 0.9712 - val_loss: 0.0773 - val_accuracy: 0.9700\n",
      "Epoch 70/250\n",
      "1001/1001 - 8s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 71/250\n",
      "1001/1001 - 8s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 72/250\n",
      "1001/1001 - 7s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0769 - val_accuracy: 0.9702\n",
      "Epoch 73/250\n",
      "1001/1001 - 7s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0767 - val_accuracy: 0.9702\n",
      "Epoch 74/250\n",
      "1001/1001 - 7s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 75/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9712 - val_loss: 0.0769 - val_accuracy: 0.9701\n",
      "Epoch 76/250\n",
      "1001/1001 - 7s - loss: 0.0743 - accuracy: 0.9712 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 77/250\n",
      "1001/1001 - 7s - loss: 0.0742 - accuracy: 0.9713 - val_loss: 0.0771 - val_accuracy: 0.9701\n",
      "Epoch 78/250\n",
      "1001/1001 - 8s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0767 - val_accuracy: 0.9702\n",
      "Epoch 79/250\n",
      "1001/1001 - 8s - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.0767 - val_accuracy: 0.9702\n",
      "Epoch 80/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0767 - val_accuracy: 0.9702\n",
      "Epoch 81/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0767 - val_accuracy: 0.9703\n",
      "Epoch 82/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 83/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0768 - val_accuracy: 0.9703\n",
      "Epoch 84/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 85/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0766 - val_accuracy: 0.9703\n",
      "Epoch 86/250\n",
      "1001/1001 - 8s - loss: 0.0739 - accuracy: 0.9713 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 87/250\n",
      "1001/1001 - 8s - loss: 0.0739 - accuracy: 0.9713 - val_loss: 0.0767 - val_accuracy: 0.9702\n",
      "Epoch 88/250\n",
      "1001/1001 - 7s - loss: 0.0739 - accuracy: 0.9713 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 89/250\n",
      "1001/1001 - 7s - loss: 0.0740 - accuracy: 0.9713 - val_loss: 0.0770 - val_accuracy: 0.9701\n",
      "Epoch 90/250\n",
      "1001/1001 - 7s - loss: 0.0741 - accuracy: 0.9713 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 91/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0769 - val_accuracy: 0.9702\n",
      "Epoch 92/250\n",
      "1001/1001 - 7s - loss: 0.0739 - accuracy: 0.9714 - val_loss: 0.0767 - val_accuracy: 0.9702\n",
      "Epoch 93/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0766 - val_accuracy: 0.9703\n",
      "Epoch 94/250\n",
      "1001/1001 - 8s - loss: 0.0738 - accuracy: 0.9713 - val_loss: 0.0766 - val_accuracy: 0.9703\n",
      "Epoch 95/250\n",
      "1001/1001 - 8s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0766 - val_accuracy: 0.9703\n",
      "Epoch 96/250\n",
      "1001/1001 - 7s - loss: 0.0737 - accuracy: 0.9714 - val_loss: 0.0766 - val_accuracy: 0.9702\n",
      "Epoch 97/250\n",
      "1001/1001 - 7s - loss: 0.0737 - accuracy: 0.9714 - val_loss: 0.0767 - val_accuracy: 0.9703\n",
      "Epoch 98/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0765 - val_accuracy: 0.9704\n",
      "Epoch 99/250\n",
      "1001/1001 - 7s - loss: 0.0737 - accuracy: 0.9714 - val_loss: 0.0766 - val_accuracy: 0.9703\n",
      "Epoch 100/250\n",
      "1001/1001 - 7s - loss: 0.0737 - accuracy: 0.9714 - val_loss: 0.0771 - val_accuracy: 0.9701\n",
      "Epoch 101/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0769 - val_accuracy: 0.9701\n",
      "Epoch 102/250\n",
      "1001/1001 - 8s - loss: 0.0737 - accuracy: 0.9714 - val_loss: 0.0765 - val_accuracy: 0.9703\n",
      "Epoch 103/250\n",
      "1001/1001 - 8s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0774 - val_accuracy: 0.9700\n",
      "Epoch 104/250\n",
      "1001/1001 - 7s - loss: 0.0738 - accuracy: 0.9714 - val_loss: 0.0766 - val_accuracy: 0.9703\n",
      "Epoch 105/250\n",
      "1001/1001 - 7s - loss: 0.0737 - accuracy: 0.9714 - val_loss: 0.0774 - val_accuracy: 0.9700\n",
      "Epoch 106/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9714 - val_loss: 0.0764 - val_accuracy: 0.9703\n",
      "Epoch 107/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9714 - val_loss: 0.0765 - val_accuracy: 0.9703\n",
      "Epoch 108/250\n",
      "1001/1001 - 8s - loss: 0.0737 - accuracy: 0.9715 - val_loss: 0.0766 - val_accuracy: 0.9703\n",
      "Epoch 109/250\n",
      "1001/1001 - 7s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0765 - val_accuracy: 0.9704\n",
      "Epoch 110/250\n",
      "1001/1001 - 8s - loss: 0.0735 - accuracy: 0.9715 - val_loss: 0.0763 - val_accuracy: 0.9704\n",
      "Epoch 111/250\n",
      "1001/1001 - 8s - loss: 0.0736 - accuracy: 0.9715 - val_loss: 0.0764 - val_accuracy: 0.9703\n",
      "Epoch 112/250\n",
      "1001/1001 - 7s - loss: 0.0734 - accuracy: 0.9715 - val_loss: 0.0764 - val_accuracy: 0.9703\n",
      "Epoch 113/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9715 - val_loss: 0.0764 - val_accuracy: 0.9704\n",
      "Epoch 114/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9715 - val_loss: 0.0765 - val_accuracy: 0.9703\n",
      "Epoch 115/250\n",
      "1001/1001 - 7s - loss: 0.0735 - accuracy: 0.9715 - val_loss: 0.0768 - val_accuracy: 0.9702\n",
      "Epoch 116/250\n",
      "1001/1001 - 7s - loss: 0.0734 - accuracy: 0.9715 - val_loss: 0.0764 - val_accuracy: 0.9703\n",
      "Epoch 117/250\n",
      "1001/1001 - 8s - loss: 0.0734 - accuracy: 0.9715 - val_loss: 0.0764 - val_accuracy: 0.9704\n",
      "Epoch 118/250\n",
      "1001/1001 - 8s - loss: 0.0735 - accuracy: 0.9715 - val_loss: 0.0763 - val_accuracy: 0.9704\n",
      "Epoch 119/250\n",
      "1001/1001 - 8s - loss: 0.0733 - accuracy: 0.9716 - val_loss: 0.0762 - val_accuracy: 0.9704\n",
      "Epoch 120/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9716 - val_loss: 0.0762 - val_accuracy: 0.9705\n",
      "Epoch 121/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9716 - val_loss: 0.0765 - val_accuracy: 0.9703\n",
      "Epoch 122/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9717 - val_loss: 0.0764 - val_accuracy: 0.9704\n",
      "Epoch 123/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9716 - val_loss: 0.0762 - val_accuracy: 0.9705\n",
      "Epoch 124/250\n",
      "1001/1001 - 7s - loss: 0.0732 - accuracy: 0.9716 - val_loss: 0.0764 - val_accuracy: 0.9704\n",
      "Epoch 125/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0761 - val_accuracy: 0.9705\n",
      "Epoch 126/250\n",
      "1001/1001 - 8s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0762 - val_accuracy: 0.9704\n",
      "Epoch 127/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0762 - val_accuracy: 0.9704\n",
      "Epoch 128/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0762 - val_accuracy: 0.9704\n",
      "Epoch 129/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0760 - val_accuracy: 0.9705\n",
      "Epoch 130/250\n",
      "1001/1001 - 7s - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.0761 - val_accuracy: 0.9705\n",
      "Epoch 131/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9717 - val_loss: 0.0759 - val_accuracy: 0.9706\n",
      "Epoch 132/250\n",
      "1001/1001 - 7s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0760 - val_accuracy: 0.9705\n",
      "Epoch 133/250\n",
      "1001/1001 - 7s - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.0759 - val_accuracy: 0.9706\n",
      "Epoch 134/250\n",
      "1001/1001 - 8s - loss: 0.0729 - accuracy: 0.9717 - val_loss: 0.0759 - val_accuracy: 0.9706\n",
      "Epoch 135/250\n",
      "1001/1001 - 8s - loss: 0.0729 - accuracy: 0.9718 - val_loss: 0.0759 - val_accuracy: 0.9706\n",
      "Epoch 136/250\n",
      "1001/1001 - 7s - loss: 0.0731 - accuracy: 0.9717 - val_loss: 0.0759 - val_accuracy: 0.9705\n",
      "Epoch 137/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9718 - val_loss: 0.0759 - val_accuracy: 0.9706\n",
      "Epoch 138/250\n",
      "1001/1001 - 7s - loss: 0.0727 - accuracy: 0.9718 - val_loss: 0.0757 - val_accuracy: 0.9706\n",
      "Epoch 139/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0756 - val_accuracy: 0.9707\n",
      "Epoch 140/250\n",
      "1001/1001 - 7s - loss: 0.0725 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 141/250\n",
      "1001/1001 - 7s - loss: 0.0724 - accuracy: 0.9719 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 142/250\n",
      "1001/1001 - 8s - loss: 0.0724 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 143/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0755 - val_accuracy: 0.9708\n",
      "Epoch 144/250\n",
      "1001/1001 - 8s - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
      "Epoch 145/250\n",
      "1001/1001 - 7s - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.0752 - val_accuracy: 0.9710\n",
      "Epoch 146/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0752 - val_accuracy: 0.9709\n",
      "Epoch 147/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.0753 - val_accuracy: 0.9709\n",
      "Epoch 148/250\n",
      "1001/1001 - 7s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0750 - val_accuracy: 0.9711\n",
      "Epoch 149/250\n",
      "1001/1001 - 7s - loss: 0.0720 - accuracy: 0.9722 - val_loss: 0.0751 - val_accuracy: 0.9710\n",
      "Epoch 150/250\n",
      "1001/1001 - 8s - loss: 0.0721 - accuracy: 0.9721 - val_loss: 0.0752 - val_accuracy: 0.9710\n",
      "Epoch 151/250\n",
      "1001/1001 - 8s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0750 - val_accuracy: 0.9711\n",
      "Epoch 152/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0750 - val_accuracy: 0.9711\n",
      "Epoch 153/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0752 - val_accuracy: 0.9711\n",
      "Epoch 154/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0749 - val_accuracy: 0.9711\n",
      "Epoch 155/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0750 - val_accuracy: 0.9710\n",
      "Epoch 156/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0749 - val_accuracy: 0.9711\n",
      "Epoch 157/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 158/250\n",
      "1001/1001 - 8s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0749 - val_accuracy: 0.9711\n",
      "Epoch 159/250\n",
      "1001/1001 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0750 - val_accuracy: 0.9711\n",
      "Epoch 160/250\n",
      "1001/1001 - 7s - loss: 0.0717 - accuracy: 0.9723 - val_loss: 0.0748 - val_accuracy: 0.9711\n",
      "Epoch 161/250\n",
      "1001/1001 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 162/250\n",
      "1001/1001 - 7s - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.0750 - val_accuracy: 0.9712\n",
      "Epoch 163/250\n",
      "1001/1001 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 164/250\n",
      "1001/1001 - 7s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 165/250\n",
      "1001/1001 - 7s - loss: 0.0715 - accuracy: 0.9723 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 166/250\n",
      "1001/1001 - 8s - loss: 0.0716 - accuracy: 0.9723 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 167/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 168/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 169/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 170/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 171/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 172/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 173/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 174/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 175/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 176/250\n",
      "1001/1001 - 8s - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 177/250\n",
      "1001/1001 - 7s - loss: 0.0714 - accuracy: 0.9724 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 178/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 179/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 180/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0750 - val_accuracy: 0.9711\n",
      "Epoch 181/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 182/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0750 - val_accuracy: 0.9712\n",
      "Epoch 183/250\n",
      "1001/1001 - 8s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 184/250\n",
      "1001/1001 - 7s - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 185/250\n",
      "1001/1001 - 7s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 186/250\n",
      "1001/1001 - 7s - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 187/250\n",
      "1001/1001 - 7s - loss: 0.0712 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 188/250\n",
      "1001/1001 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 189/250\n",
      "1001/1001 - 7s - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 190/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 191/250\n",
      "1001/1001 - 8s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 192/250\n",
      "1001/1001 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 193/250\n",
      "1001/1001 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 194/250\n",
      "1001/1001 - 7s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 195/250\n",
      "1001/1001 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 196/250\n",
      "1001/1001 - 7s - loss: 0.0718 - accuracy: 0.9722 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 197/250\n",
      "1001/1001 - 7s - loss: 0.0711 - accuracy: 0.9725 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 198/250\n",
      "1001/1001 - 8s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 199/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "\n",
      "Epoch 00200: starting stochastic weight averaging\n",
      "Epoch 200/250\n",
      "1001/1001 - 8s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 201/250\n",
      "1001/1001 - 7s - loss: 0.0709 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 202/250\n",
      "1001/1001 - 8s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 203/250\n",
      "1001/1001 - 7s - loss: 0.0709 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 204/250\n",
      "1001/1001 - 7s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 205/250\n",
      "1001/1001 - 7s - loss: 0.0709 - accuracy: 0.9726 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 206/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 207/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0746 - val_accuracy: 0.9713\n",
      "Epoch 208/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 209/250\n",
      "1001/1001 - 8s - loss: 0.0708 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 210/250\n",
      "1001/1001 - 8s - loss: 0.0708 - accuracy: 0.9726 - val_loss: 0.0749 - val_accuracy: 0.9713\n",
      "Epoch 211/250\n",
      "1001/1001 - 7s - loss: 0.0708 - accuracy: 0.9726 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 212/250\n",
      "1001/1001 - 7s - loss: 0.0708 - accuracy: 0.9726 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 213/250\n",
      "1001/1001 - 7s - loss: 0.0708 - accuracy: 0.9726 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 214/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9726 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 215/250\n",
      "1001/1001 - 8s - loss: 0.0707 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 216/250\n",
      "1001/1001 - 8s - loss: 0.0707 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 217/250\n",
      "1001/1001 - 8s - loss: 0.0709 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 218/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9727 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 219/250\n",
      "1001/1001 - 7s - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 220/250\n",
      "1001/1001 - 7s - loss: 0.0706 - accuracy: 0.9727 - val_loss: 0.0747 - val_accuracy: 0.9712\n",
      "Epoch 221/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 222/250\n",
      "1001/1001 - 8s - loss: 0.0707 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 223/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9727 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 224/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 225/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9727 - val_loss: 0.0749 - val_accuracy: 0.9711\n",
      "Epoch 226/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9727 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 227/250\n",
      "1001/1001 - 7s - loss: 0.0708 - accuracy: 0.9725 - val_loss: 0.0754 - val_accuracy: 0.9709\n",
      "Epoch 228/250\n",
      "1001/1001 - 7s - loss: 0.0706 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 229/250\n",
      "1001/1001 - 8s - loss: 0.0705 - accuracy: 0.9727 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 230/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9726 - val_loss: 0.0747 - val_accuracy: 0.9714\n",
      "Epoch 231/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9727 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 232/250\n",
      "1001/1001 - 7s - loss: 0.0705 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 233/250\n",
      "1001/1001 - 7s - loss: 0.0706 - accuracy: 0.9726 - val_loss: 0.0750 - val_accuracy: 0.9711\n",
      "Epoch 234/250\n",
      "1001/1001 - 8s - loss: 0.0706 - accuracy: 0.9727 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 235/250\n",
      "1001/1001 - 7s - loss: 0.0705 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 236/250\n",
      "1001/1001 - 7s - loss: 0.0704 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 237/250\n",
      "1001/1001 - 8s - loss: 0.0705 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 238/250\n",
      "1001/1001 - 8s - loss: 0.0705 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 239/250\n",
      "1001/1001 - 8s - loss: 0.0704 - accuracy: 0.9728 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 240/250\n",
      "1001/1001 - 8s - loss: 0.0704 - accuracy: 0.9727 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 241/250\n",
      "1001/1001 - 7s - loss: 0.0704 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 242/250\n",
      "1001/1001 - 8s - loss: 0.0703 - accuracy: 0.9728 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 243/250\n",
      "1001/1001 - 8s - loss: 0.0705 - accuracy: 0.9727 - val_loss: 0.0750 - val_accuracy: 0.9712\n",
      "Epoch 244/250\n",
      "1001/1001 - 7s - loss: 0.0705 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9713\n",
      "Epoch 245/250\n",
      "1001/1001 - 7s - loss: 0.0704 - accuracy: 0.9727 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 246/250\n",
      "1001/1001 - 8s - loss: 0.0703 - accuracy: 0.9728 - val_loss: 0.0750 - val_accuracy: 0.9711\n",
      "Epoch 247/250\n",
      "1001/1001 - 8s - loss: 0.0703 - accuracy: 0.9728 - val_loss: 0.0747 - val_accuracy: 0.9713\n",
      "Epoch 248/250\n",
      "1001/1001 - 8s - loss: 0.0703 - accuracy: 0.9728 - val_loss: 0.0749 - val_accuracy: 0.9712\n",
      "Epoch 249/250\n",
      "1001/1001 - 7s - loss: 0.0703 - accuracy: 0.9727 - val_loss: 0.0748 - val_accuracy: 0.9712\n",
      "Epoch 250/250\n",
      "1001/1001 - 8s - loss: 0.0703 - accuracy: 0.9728 - val_loss: 0.0750 - val_accuracy: 0.9711\n",
      "\n",
      "Epoch 00251: final model weights set to stochastic weight average\n",
      "Training fold 5 completed. macro f1 score : 0.94268\n",
      "Training completed. oof macro f1 score : 0.94246\n",
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n",
    "run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\n",
    "print('Training completed...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
